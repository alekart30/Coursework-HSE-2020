{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coursework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FUKtJYfd3o0C",
        "d7uemNenL7j3",
        "JprvR7EJBezc",
        "3pYRihYqv4qp",
        "syvZyqc0JoxQ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55d4f3f134d045adba8150b25a44a325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_941ff7770b5940cb96e12c0435181596",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c91bc7454104b4abdf3bb96c0ff2394",
              "IPY_MODEL_9b6c9f3806ca4a9e8356540c5840aff9"
            ]
          }
        },
        "941ff7770b5940cb96e12c0435181596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c91bc7454104b4abdf3bb96c0ff2394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f2d1b23e0c5f416db14bed650e0b5f30",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 15,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24510eac06374e79af27ee4dd0b2567b"
          }
        },
        "9b6c9f3806ca4a9e8356540c5840aff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a62daf19d3b4d3aa1eea9734159aa63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15/15 [00:00&lt;00:00, 35.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5c9a804b9cc483c805548710eff2097"
          }
        },
        "f2d1b23e0c5f416db14bed650e0b5f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24510eac06374e79af27ee4dd0b2567b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a62daf19d3b4d3aa1eea9734159aa63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5c9a804b9cc483c805548710eff2097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e0631cee3974e4bb9498cb981976394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6648b88785a44ad9dc0106b400f53ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b5a5db44996c43ba87301753210b9edf",
              "IPY_MODEL_55b46316404d409eac4579d8fa26f165"
            ]
          }
        },
        "e6648b88785a44ad9dc0106b400f53ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5a5db44996c43ba87301753210b9edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_24d9e7a7f11f4e6e9fc5998894374cb7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21a7f82ad6094707b671b68947c5b38d"
          }
        },
        "55b46316404d409eac4579d8fa26f165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ac89d4ef378492886aac25ee0815c69",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [00:02&lt;00:00, 36.93it/s, correct=92.68%, skipped=16.07%]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a83eb6973fff42ac82a8cb7339431224"
          }
        },
        "24d9e7a7f11f4e6e9fc5998894374cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21a7f82ad6094707b671b68947c5b38d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ac89d4ef378492886aac25ee0815c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a83eb6973fff42ac82a8cb7339431224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqnowGiDVu98",
        "colab_type": "text"
      },
      "source": [
        "# All imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-4pdpEUSCEs",
        "colab_type": "code",
        "outputId": "1f0e615c-ced9-4137-f816-ecc591829464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "! pip install implicit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting implicit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/d8/6b4f1374ffa2647b72ac76960c71b984c6f3238090359fb419d03827d87a/implicit-0.4.2.tar.gz (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 6.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 6.2MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 7.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 6.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 6.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 6.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 921kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from implicit) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from implicit) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from implicit) (4.38.0)\n",
            "Building wheels for collected packages: implicit\n",
            "  Building wheel for implicit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for implicit: filename=implicit-0.4.2-cp36-cp36m-linux_x86_64.whl size=3472942 sha256=06e413dabce426ed3e390e5a984d475f96a3b87c04f810b4527ae4136e2498e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/48/b1/1aebe3acc3afb5589e72d3e7c3ffc3f637dc4721c1a974dff7\n",
            "Successfully built implicit\n",
            "Installing collected packages: implicit\n",
            "Successfully installed implicit-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_umyp1QSGS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import implicit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyl4H5mbVZnr",
        "colab_type": "code",
        "outputId": "43e9327b-23fa-4930-f6ee-1053ffe8c836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix, coo_matrix\n",
        "\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScFVeiSuX_yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "sns.set_style('darkgrid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afsz8vVRV0EG",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqs21RL_Vo_E",
        "colab_type": "code",
        "outputId": "88ade438-b037-42c0-f699-e08116da7843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "if not os.path.exists('./data'):\n",
        "  os.mkdir('./data')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1r6U_EFULKXiuLPrIEAdoJ7W_1Xa7aJ3Q', dest_path='./data/movies.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1r6U_EFULKXiuLPrIEAdoJ7W_1Xa7aJ3Q into ./data/movies.zip... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU61I9hhV2l-",
        "colab_type": "code",
        "outputId": "69656a09-b5f4-48e9-cce9-f5623d8b9412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "! unzip ./data/movies.zip -d ./data/unziped"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./data/movies.zip\n",
            "  inflating: ./data/unziped/links.csv  \n",
            "  inflating: ./data/unziped/movies.csv  \n",
            "  inflating: ./data/unziped/ratings.csv  \n",
            "  inflating: ./data/unziped/tags.csv  \n",
            "  inflating: ./data/unziped/u.data   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mko2Cu6rWCiw",
        "colab_type": "text"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s7r-RqNWEvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ratings_df = pd.read_csv('/content/data/unziped/ratings.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0qEc6jWWUxX",
        "colab_type": "code",
        "outputId": "f7a89d81-fcf6-4d5b-9b9a-6ebcf29000c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "ratings_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1260759144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1029</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1260759179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1061</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1260759182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1129</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1260759185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1172</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1260759205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating   timestamp\n",
              "0       1       31     2.5  1260759144\n",
              "1       1     1029     3.0  1260759179\n",
              "2       1     1061     3.0  1260759182\n",
              "3       1     1129     2.0  1260759185\n",
              "4       1     1172     4.0  1260759205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgKl5JaXWYmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], unit='s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN77ooQgWhH3",
        "colab_type": "code",
        "outputId": "a6d50726-a713-4da8-d54a-6b3e86c8b756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "ratings_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2009-12-14 02:52:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1029</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2009-12-14 02:52:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1061</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2009-12-14 02:53:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1129</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2009-12-14 02:53:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1172</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2009-12-14 02:53:25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating           timestamp\n",
              "0       1       31     2.5 2009-12-14 02:52:24\n",
              "1       1     1029     3.0 2009-12-14 02:52:59\n",
              "2       1     1061     3.0 2009-12-14 02:53:02\n",
              "3       1     1129     2.0 2009-12-14 02:53:05\n",
              "4       1     1172     4.0 2009-12-14 02:53:25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31XEtjrwW5pk",
        "colab_type": "text"
      },
      "source": [
        "Check nans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwC970ybWtjQ",
        "colab_type": "code",
        "outputId": "b069e6fc-ef22-49fe-9afb-0185b2af308c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "ratings_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100004 entries, 0 to 100003\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count   Dtype         \n",
            "---  ------     --------------   -----         \n",
            " 0   userId     100004 non-null  int64         \n",
            " 1   movieId    100004 non-null  int64         \n",
            " 2   rating     100004 non-null  float64       \n",
            " 3   timestamp  100004 non-null  datetime64[ns]\n",
            "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
            "memory usage: 3.1 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVHLXh5YW7Od",
        "colab_type": "code",
        "outputId": "ed35201e-877f-49bb-aba0-eaf587600811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ratings_df['userId'].nunique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "671"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbkCKjacXEm1",
        "colab_type": "code",
        "outputId": "0ce34ab2-9b94-4be1-e840-b61d4d3c2f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ratings_df['movieId'].nunique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAi-KXIAXNAd",
        "colab_type": "text"
      },
      "source": [
        "Sparcity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mwY9rF7XOXJ",
        "colab_type": "code",
        "outputId": "9c109c70-0828-4ce6-eb18-c9a773e0e6f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ratings_df.shape[0] / (ratings_df['userId'].nunique() * ratings_df['movieId'].nunique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.016439141608663475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4ilS806XYQI",
        "colab_type": "text"
      },
      "source": [
        "Ratings distribution over users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2p8gdAkXcXI",
        "colab_type": "code",
        "outputId": "0672dcc7-3627-4f80-ed30-53278a869d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "sns.distplot(ratings_df.groupby('userId').agg({'rating':'count'}),ax=ax,kde=False)\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel(\"number of ratings per user\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'number of ratings per user')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBABgOYIAACxHEACA5QgCALDc/wCYhUre3SAaAQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4-NbsBzY9bH",
        "colab_type": "text"
      },
      "source": [
        "Ratings distribution over movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_v8KFHTZBAv",
        "colab_type": "code",
        "outputId": "01ba889f-041d-4757-e6dc-bc469d3d0b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "sns.distplot(ratings_df.groupby('movieId').agg({'rating':'count'}),ax=ax,kde=False)\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel(\"number of ratings per movie\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'number of ratings per movie')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEMCAYAAAAh7MZPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhiDHkiIokx5ImIJPZ/mf7Z9wtwjacAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPzHWJLnZIj1",
        "colab_type": "text"
      },
      "source": [
        "# Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odA6axcUZQqo",
        "colab_type": "text"
      },
      "source": [
        "Let's transform it from explicit to implicit due to following rule:\n",
        "\n",
        "1 if we have rating for pair (user, movie), 0 otherwise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjdZ-jcuZK0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ratings_df['rating'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIsP0x1o2YvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ratings_per_user = ratings_df.groupby('userId').agg({'rating':'count'}).reset_index().rename(columns={'rating':'rating_num'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BevADGkL2rPB",
        "colab_type": "text"
      },
      "source": [
        "There is no need in removing any users, there are no cold users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67tXh94S2Z1e",
        "colab_type": "code",
        "outputId": "7de78ca8-1963-4387-b5ab-7619278d583b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ratings_per_user['rating_num'].min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBb4hqEH209B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ratings_per_movie = ratings_df.groupby('movieId').agg({'rating':'count'}).reset_index().rename(columns={'rating':'rating_num'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7zXOTa_25gJ",
        "colab_type": "code",
        "outputId": "5ec8277d-a350-4ba6-f926-fa5b66d70c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ratings_per_movie['rating_num'].min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ArPHTfh9q6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ratings_df.rename(columns={'movieId':'itemId'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ruu9gZk2Cvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classical_data_split(X):\n",
        "  train_and_val_df, test_df = train_test_split(X, test_size=0.1)\n",
        "  train_df, val_df = train_test_split(train_and_val_df, train_size=7/9)\n",
        "  return train_df, val_df, test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i849ILlb3fiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, val_df, test_df = classical_data_split(ratings_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUKtJYfd3o0C",
        "colab_type": "text"
      },
      "source": [
        "# Most popular recommender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAh3P5aZ3gxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MostPopularRecommender():\n",
        "  def __init__(self):\n",
        "    self.sorted_items = None\n",
        "\n",
        "  def fit(self, X):\n",
        "    \"\"\"Build list of items according to their popularity\n",
        "\n",
        "    :param X: incoming data for fitting contains 'userId' and 'itemId' columns (DataFrame)\n",
        "    \"\"\"\n",
        "    self.sorted_items = X.groupby('itemId').agg({'rating':'count'}).sort_values(by='rating', ascending=False).reset_index()['itemId'].tolist()\n",
        "  \n",
        "  def predict(self, X, top_n):\n",
        "    \"\"\"Predict top_n popular items for each user\n",
        "    \n",
        "    :param X: incoming data for prediction contains 'userId' column (DataFrame)\n",
        "    :param top_n: number of recommendations (int)\n",
        "    :return: items recommendations for each user (DataFrame)\n",
        "    \"\"\"\n",
        "    prediction_df = pd.DataFrame()\n",
        "    prediction_df['userId'] = X['userId'].unique()\n",
        "    prediction_df['rec_items'] = [self.sorted_items[:top_n] for _ in range(X['userId'].nunique())]\n",
        "    return prediction_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7uemNenL7j3",
        "colab_type": "text"
      },
      "source": [
        "# KNN User-based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F12-FbhLZc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KNNUserBased():\n",
        "  def __init__(self, n_neighbors):\n",
        "    self.n_neighbors = n_neighbors\n",
        "    self.user_item_matrix = None\n",
        "    self.model_knn = None\n",
        "\n",
        "  def fit(self, X):\n",
        "    \"\"\"Build user-item matrix and train KNN for users\n",
        "\n",
        "    :param X: incoming data for fitting contains 'userId', 'itemId' and 'rating' columns (DataFrame)\n",
        "    \"\"\"\n",
        "    self.user_item_matrix = X.pivot(index='userId',\n",
        "                               columns='itemId',\n",
        "                               values='rating'\n",
        "                              ).fillna(0)\n",
        "    self.model_knn = NearestNeighbors(metric='cosine', algorithm='kd_tree', n_neighbors=self.n_neighbors, n_jobs=-1)\n",
        "    self.model_knn.fit(self.user_item_matrix)\n",
        "\n",
        "  def predict(self, X):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4PqEfxV8M-L",
        "colab_type": "code",
        "outputId": "48bd1dc2-b334-45bf-8971-93424f19b50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "user_item_matrix = train_df.pivot(index='userId',\n",
        "                               columns='itemId',\n",
        "                               values='rating'\n",
        "                              ).fillna(0).values\n",
        "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10, n_jobs=-1)\n",
        "model_knn.fit(user_item_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine',\n",
              "                 metric_params=None, n_jobs=-1, n_neighbors=10, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqkldImD_o5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_to_idx = {u:idx for idx, u in enumerate(sorted(train_df['userId'].unique()))}\n",
        "idx_to_user = {idx:u for u, idx in user_to_idx.items()}\n",
        "item_to_idx = {i:idx for idx, i in enumerate(sorted(train_df['itemId'].unique()))}\n",
        "idx_to_item = {idx:i for i, idx in item_to_idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbesQdiytoBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_df = pd.DataFrame()\n",
        "predicted_df['userId'] = list(set(test_df['userId'].unique()) & set(train_df['userId'].unique()))\n",
        "predicted_df['rec_items'] = 0\n",
        "predicted_df.set_index('userId', inplace=True)\n",
        "predicted_df = predicted_df.astype('object')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqIecqEi8cA_",
        "colab_type": "code",
        "outputId": "50c6837a-c5d9-44be-acbb-9d3005b8beb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for user in tqdm(test_df['userId'].unique()):\n",
        "  if user in train_df['userId'].unique():\n",
        "    user_vector = user_item_matrix[user_to_idx[user],:]\n",
        "    distances, neighbours = model_knn.kneighbors(user_vector.reshape(1,-1))\n",
        "    similar_users_indices = neighbours[0].tolist()\n",
        "    distances_tiled = np.tile(distances[0], train_df['itemId'].nunique()).reshape(10,train_df['itemId'].nunique())\n",
        "    scores = np.sum(user_item_matrix[similar_users_indices,:] * distances_tiled, axis=0) / np.sum(distances[0])\n",
        "    best_items_idx = scores.argsort()[::-1].tolist()\n",
        "    best_items = []\n",
        "    for item_idx in best_items_idx:\n",
        "      if item_idx not in train_df[train_df['userId'] == user]['itemId'].unique():\n",
        "        best_items.append(idx_to_item[item_idx])\n",
        "      if len(best_items) == 5:\n",
        "        break\n",
        "\n",
        "    #best_items = [idx_to_item[idx] for idx in best_items_idx]\n",
        "    predicted_df.at[user, 'rec_items'] = best_items\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/659 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 1/659 [00:00<01:19,  8.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 2/659 [00:00<01:22,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 3/659 [00:00<01:23,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 4/659 [00:00<01:23,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 5/659 [00:00<01:23,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 6/659 [00:00<01:21,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 7/659 [00:00<01:20,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 8/659 [00:01<01:20,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|▏         | 9/659 [00:01<01:21,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 10/659 [00:01<01:22,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 11/659 [00:01<01:22,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 12/659 [00:01<01:22,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 13/659 [00:01<01:21,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 14/659 [00:01<01:22,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 15/659 [00:01<01:22,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 16/659 [00:02<01:21,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|▎         | 17/659 [00:02<01:22,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|▎         | 18/659 [00:02<01:21,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|▎         | 19/659 [00:02<01:21,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|▎         | 20/659 [00:02<01:21,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|▎         | 21/659 [00:02<01:21,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|▎         | 22/659 [00:02<01:21,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|▎         | 23/659 [00:02<01:20,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▎         | 24/659 [00:03<01:19,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 25/659 [00:03<01:19,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 26/659 [00:03<01:20,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 27/659 [00:03<01:18,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 28/659 [00:03<01:19,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 29/659 [00:03<01:19,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|▍         | 30/659 [00:03<01:18,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|▍         | 31/659 [00:03<01:19,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|▍         | 32/659 [00:04<01:17,  8.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|▌         | 33/659 [00:04<01:17,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|▌         | 34/659 [00:04<01:19,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|▌         | 35/659 [00:04<01:17,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|▌         | 36/659 [00:04<01:17,  8.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 37/659 [00:04<01:17,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 38/659 [00:04<01:18,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 39/659 [00:04<01:16,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 40/659 [00:05<01:17,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 41/659 [00:05<01:18,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▋         | 42/659 [00:05<01:21,  7.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 43/659 [00:05<01:22,  7.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 44/659 [00:05<01:22,  7.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 45/659 [00:05<01:21,  7.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 46/659 [00:05<01:20,  7.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 47/659 [00:05<01:20,  7.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 48/659 [00:06<01:19,  7.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 49/659 [00:06<01:19,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 50/659 [00:06<01:17,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 51/659 [00:06<01:17,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 52/659 [00:06<01:17,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 53/659 [00:06<01:17,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 54/659 [00:06<01:15,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 55/659 [00:06<01:15,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 56/659 [00:07<01:14,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|▊         | 57/659 [00:07<01:17,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|▉         | 58/659 [00:07<01:17,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|▉         | 59/659 [00:07<01:15,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|▉         | 60/659 [00:07<01:15,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|▉         | 61/659 [00:07<01:15,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|▉         | 62/659 [00:07<01:15,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|▉         | 63/659 [00:08<01:14,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|▉         | 64/659 [00:08<01:15,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|▉         | 65/659 [00:08<01:14,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 66/659 [00:08<01:15,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 67/659 [00:08<01:14,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 68/659 [00:08<01:14,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 69/659 [00:08<01:15,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|█         | 70/659 [00:08<01:14,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|█         | 71/659 [00:09<01:17,  7.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|█         | 72/659 [00:09<01:16,  7.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|█         | 73/659 [00:09<01:16,  7.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|█         | 74/659 [00:09<01:17,  7.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|█▏        | 75/659 [00:09<01:16,  7.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 76/659 [00:09<01:15,  7.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 77/659 [00:09<01:15,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 78/659 [00:09<01:14,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 79/659 [00:10<01:14,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 80/659 [00:10<01:14,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 81/659 [00:10<01:14,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 82/659 [00:10<01:14,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 13%|█▎        | 83/659 [00:10<01:14,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 13%|█▎        | 84/659 [00:10<01:13,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 13%|█▎        | 85/659 [00:10<01:13,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 13%|█▎        | 86/659 [00:10<01:12,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 13%|█▎        | 87/659 [00:11<01:13,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 13%|█▎        | 88/659 [00:11<01:12,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▎        | 89/659 [00:11<01:13,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▎        | 90/659 [00:11<01:12,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 91/659 [00:11<01:12,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 92/659 [00:11<01:12,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 93/659 [00:11<01:10,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 94/659 [00:11<01:10,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 95/659 [00:12<01:09,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|█▍        | 96/659 [00:12<01:09,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|█▍        | 97/659 [00:12<01:11,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|█▍        | 98/659 [00:12<01:11,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|█▌        | 99/659 [00:12<01:10,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|█▌        | 100/659 [00:12<01:10,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|█▌        | 101/659 [00:12<01:10,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|█▌        | 102/659 [00:12<01:09,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 103/659 [00:13<01:09,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 104/659 [00:13<01:10,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 105/659 [00:13<01:09,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 106/659 [00:13<01:09,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 107/659 [00:13<01:09,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▋        | 108/659 [00:13<01:09,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 109/659 [00:13<01:09,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 110/659 [00:13<01:08,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 111/659 [00:14<01:08,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 112/659 [00:14<01:09,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 113/659 [00:14<01:10,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 114/659 [00:14<01:09,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 115/659 [00:14<01:09,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 116/659 [00:14<01:07,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 117/659 [00:14<01:07,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 118/659 [00:15<01:07,  8.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 119/659 [00:15<01:08,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 120/659 [00:15<01:08,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 121/659 [00:15<01:08,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▊        | 122/659 [00:15<01:09,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▊        | 123/659 [00:15<01:07,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▉        | 124/659 [00:15<01:07,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▉        | 125/659 [00:15<01:07,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▉        | 126/659 [00:16<01:09,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▉        | 127/659 [00:16<01:09,  7.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▉        | 128/659 [00:16<01:08,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|█▉        | 129/659 [00:16<01:08,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|█▉        | 130/659 [00:16<01:08,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|█▉        | 131/659 [00:16<01:07,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 132/659 [00:16<01:08,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 133/659 [00:16<01:07,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 134/659 [00:17<01:08,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 135/659 [00:17<01:08,  7.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 136/659 [00:17<01:08,  7.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 137/659 [00:17<01:07,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 138/659 [00:17<01:06,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 139/659 [00:17<01:06,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 140/659 [00:17<01:07,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██▏       | 141/659 [00:17<01:07,  7.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 142/659 [00:18<01:06,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 143/659 [00:18<01:05,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 144/659 [00:18<01:03,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 145/659 [00:18<01:02,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 146/659 [00:18<01:03,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 147/659 [00:18<01:04,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 148/659 [00:18<01:04,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 23%|██▎       | 149/659 [00:18<01:05,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 23%|██▎       | 150/659 [00:19<01:04,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 23%|██▎       | 151/659 [00:19<01:02,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 23%|██▎       | 152/659 [00:19<01:02,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 23%|██▎       | 153/659 [00:19<01:02,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 23%|██▎       | 154/659 [00:19<01:03,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▎       | 155/659 [00:19<01:02,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▎       | 156/659 [00:19<01:02,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 157/659 [00:19<01:01,  8.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 158/659 [00:20<01:01,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 159/659 [00:20<01:01,  8.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 160/659 [00:20<01:01,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 161/659 [00:20<01:01,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▍       | 162/659 [00:20<01:02,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▍       | 163/659 [00:20<01:01,  8.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▍       | 164/659 [00:20<01:00,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▌       | 165/659 [00:20<01:01,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▌       | 166/659 [00:21<01:01,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▌       | 167/659 [00:21<01:02,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▌       | 168/659 [00:21<01:01,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 169/659 [00:21<01:01,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 170/659 [00:21<01:01,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 171/659 [00:21<01:02,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 172/659 [00:21<01:01,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▋       | 173/659 [00:21<01:01,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▋       | 174/659 [00:22<01:01,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|██▋       | 175/659 [00:22<01:00,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|██▋       | 176/659 [00:22<01:00,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|██▋       | 177/659 [00:22<01:00,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|██▋       | 178/659 [00:22<01:00,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|██▋       | 179/659 [00:22<00:59,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|██▋       | 180/659 [00:22<00:59,  8.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|██▋       | 181/659 [00:22<00:58,  8.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 182/659 [00:23<00:58,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 183/659 [00:23<00:58,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 184/659 [00:23<00:59,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 185/659 [00:23<00:58,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 186/659 [00:23<01:01,  7.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 187/659 [00:23<01:00,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▊       | 188/659 [00:23<01:00,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▊       | 189/659 [00:23<01:00,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▉       | 190/659 [00:24<00:59,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▉       | 191/659 [00:24<00:57,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▉       | 192/659 [00:24<00:56,  8.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▉       | 193/659 [00:24<00:56,  8.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▉       | 194/659 [00:24<00:55,  8.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|██▉       | 195/659 [00:24<00:55,  8.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|██▉       | 196/659 [00:24<00:56,  8.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|██▉       | 197/659 [00:24<00:57,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 198/659 [00:25<00:57,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 199/659 [00:25<00:57,  8.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 200/659 [00:25<00:57,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|███       | 201/659 [00:25<00:57,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|███       | 202/659 [00:25<00:56,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|███       | 203/659 [00:25<00:56,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|███       | 204/659 [00:25<00:56,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|███       | 205/659 [00:25<00:56,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|███▏      | 206/659 [00:26<00:57,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|███▏      | 207/659 [00:26<00:56,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 208/659 [00:26<00:55,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 209/659 [00:26<00:56,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 210/659 [00:26<00:55,  8.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 211/659 [00:26<00:55,  8.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 212/659 [00:26<00:54,  8.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 213/659 [00:26<00:55,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 214/659 [00:27<00:55,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 215/659 [00:27<00:56,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 216/659 [00:27<00:56,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 217/659 [00:27<00:56,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 218/659 [00:27<00:56,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 219/659 [00:27<00:57,  7.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 220/659 [00:27<00:58,  7.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▎      | 221/659 [00:27<00:57,  7.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▎      | 222/659 [00:28<00:57,  7.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 223/659 [00:28<00:56,  7.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 224/659 [00:28<00:56,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 225/659 [00:28<00:57,  7.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 226/659 [00:28<00:57,  7.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 227/659 [00:28<00:56,  7.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|███▍      | 228/659 [00:28<00:56,  7.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|███▍      | 229/659 [00:29<00:56,  7.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|███▍      | 230/659 [00:29<00:55,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|███▌      | 231/659 [00:29<00:55,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|███▌      | 232/659 [00:29<00:55,  7.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|███▌      | 233/659 [00:29<00:54,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 234/659 [00:29<00:53,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 235/659 [00:29<00:53,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 236/659 [00:29<00:53,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 237/659 [00:30<00:52,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 238/659 [00:30<00:53,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▋      | 239/659 [00:30<00:53,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▋      | 240/659 [00:30<00:53,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 241/659 [00:30<00:53,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 242/659 [00:30<00:53,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 243/659 [00:30<00:54,  7.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 244/659 [00:30<00:54,  7.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 245/659 [00:31<00:53,  7.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 246/659 [00:31<00:53,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 247/659 [00:31<00:53,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 248/659 [00:31<00:51,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 249/659 [00:31<00:51,  8.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 250/659 [00:31<00:50,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 251/659 [00:31<00:51,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 252/659 [00:31<00:51,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 253/659 [00:32<00:52,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|███▊      | 254/659 [00:32<00:52,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|███▊      | 255/659 [00:32<00:52,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|███▉      | 256/659 [00:32<00:51,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|███▉      | 257/659 [00:32<00:51,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|███▉      | 258/659 [00:32<00:50,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|███▉      | 259/659 [00:32<00:52,  7.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|███▉      | 260/659 [00:32<00:50,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|███▉      | 261/659 [00:33<00:51,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|███▉      | 262/659 [00:33<00:50,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|███▉      | 263/659 [00:33<00:49,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 264/659 [00:33<00:49,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 265/659 [00:33<00:48,  8.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 266/659 [00:33<00:48,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|████      | 267/659 [00:33<00:48,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|████      | 268/659 [00:33<00:49,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|████      | 269/659 [00:34<00:49,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|████      | 270/659 [00:34<00:49,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|████      | 271/659 [00:34<00:49,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|████▏     | 272/659 [00:34<00:49,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|████▏     | 273/659 [00:34<00:49,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 274/659 [00:34<00:48,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 275/659 [00:34<00:49,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 276/659 [00:35<00:48,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 277/659 [00:35<00:48,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 278/659 [00:35<00:48,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 279/659 [00:35<00:48,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 280/659 [00:35<00:48,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|████▎     | 281/659 [00:35<00:47,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|████▎     | 282/659 [00:35<00:48,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|████▎     | 283/659 [00:35<00:50,  7.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|████▎     | 284/659 [00:36<00:49,  7.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|████▎     | 285/659 [00:36<00:48,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|████▎     | 286/659 [00:36<00:47,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▎     | 287/659 [00:36<00:47,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▎     | 288/659 [00:36<00:47,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 289/659 [00:36<00:47,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 290/659 [00:36<00:48,  7.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 291/659 [00:36<00:48,  7.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 292/659 [00:37<00:48,  7.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 293/659 [00:37<00:47,  7.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|████▍     | 294/659 [00:37<00:47,  7.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|████▍     | 295/659 [00:37<00:46,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|████▍     | 296/659 [00:37<00:46,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|████▌     | 297/659 [00:37<00:46,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|████▌     | 298/659 [00:37<00:45,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|████▌     | 299/659 [00:37<00:44,  8.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 300/659 [00:38<00:43,  8.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 301/659 [00:38<00:44,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 302/659 [00:38<00:45,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 303/659 [00:38<00:45,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 304/659 [00:38<00:45,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▋     | 305/659 [00:38<00:45,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▋     | 306/659 [00:38<00:45,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|████▋     | 307/659 [00:38<00:45,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|████▋     | 308/659 [00:39<00:45,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|████▋     | 309/659 [00:39<00:45,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|████▋     | 310/659 [00:39<00:44,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|████▋     | 311/659 [00:39<00:44,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|████▋     | 312/659 [00:39<00:44,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|████▋     | 313/659 [00:39<00:43,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 314/659 [00:39<00:43,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 315/659 [00:40<00:43,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 316/659 [00:40<00:43,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 317/659 [00:40<00:43,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 318/659 [00:40<00:43,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 319/659 [00:40<00:43,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|████▊     | 320/659 [00:40<00:43,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|████▊     | 321/659 [00:40<00:41,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|████▉     | 322/659 [00:40<00:41,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|████▉     | 323/659 [00:40<00:41,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|████▉     | 324/659 [00:41<00:41,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|████▉     | 325/659 [00:41<00:41,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|████▉     | 326/659 [00:41<00:41,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|████▉     | 327/659 [00:41<00:41,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|████▉     | 328/659 [00:41<00:41,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|████▉     | 329/659 [00:41<00:41,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 330/659 [00:41<00:41,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 331/659 [00:41<00:40,  8.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 332/659 [00:42<00:40,  8.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████     | 333/659 [00:42<00:40,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████     | 334/659 [00:42<00:39,  8.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████     | 335/659 [00:42<00:40,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████     | 336/659 [00:42<00:40,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████     | 337/659 [00:42<00:40,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████▏    | 338/659 [00:42<00:41,  7.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████▏    | 339/659 [00:43<00:41,  7.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 340/659 [00:43<00:41,  7.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 341/659 [00:43<00:40,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 342/659 [00:43<00:39,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 343/659 [00:43<00:38,  8.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 344/659 [00:43<00:38,  8.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 345/659 [00:43<00:37,  8.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 346/659 [00:43<00:37,  8.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 347/659 [00:43<00:37,  8.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 348/659 [00:44<00:36,  8.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 349/659 [00:44<00:37,  8.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 350/659 [00:44<00:37,  8.28it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 351/659 [00:44<00:37,  8.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 352/659 [00:44<00:38,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▎    | 353/659 [00:44<00:38,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▎    | 354/659 [00:44<00:38,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 355/659 [00:44<00:37,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 356/659 [00:45<00:37,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 357/659 [00:45<00:38,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 358/659 [00:45<00:38,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 359/659 [00:45<00:37,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|█████▍    | 360/659 [00:45<00:37,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|█████▍    | 361/659 [00:45<00:37,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|█████▍    | 362/659 [00:45<00:37,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|█████▌    | 363/659 [00:45<00:38,  7.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|█████▌    | 364/659 [00:46<00:38,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|█████▌    | 365/659 [00:46<00:37,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 366/659 [00:46<00:37,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 367/659 [00:46<00:37,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 368/659 [00:46<00:37,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 369/659 [00:46<00:36,  8.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 370/659 [00:46<00:35,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▋    | 371/659 [00:46<00:34,  8.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▋    | 372/659 [00:47<00:34,  8.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|█████▋    | 373/659 [00:47<00:35,  8.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|█████▋    | 374/659 [00:47<00:35,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|█████▋    | 375/659 [00:47<00:35,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|█████▋    | 376/659 [00:47<00:35,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|█████▋    | 377/659 [00:47<00:35,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|█████▋    | 378/659 [00:47<00:34,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 379/659 [00:47<00:34,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 380/659 [00:48<00:34,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 381/659 [00:48<00:34,  8.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 382/659 [00:48<00:34,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 383/659 [00:48<00:33,  8.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 384/659 [00:48<00:34,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 385/659 [00:48<00:34,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▊    | 386/659 [00:48<00:34,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▊    | 387/659 [00:48<00:34,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▉    | 388/659 [00:49<00:34,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▉    | 389/659 [00:49<00:34,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▉    | 390/659 [00:49<00:34,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▉    | 391/659 [00:49<00:34,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▉    | 392/659 [00:49<00:34,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|█████▉    | 393/659 [00:49<00:34,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|█████▉    | 394/659 [00:49<00:33,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|█████▉    | 395/659 [00:50<00:33,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 396/659 [00:50<00:33,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 397/659 [00:50<00:33,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 398/659 [00:50<00:33,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████    | 399/659 [00:50<00:33,  7.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████    | 400/659 [00:50<00:33,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████    | 401/659 [00:50<00:32,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████    | 402/659 [00:50<00:32,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████    | 403/659 [00:51<00:31,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████▏   | 404/659 [00:51<00:31,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████▏   | 405/659 [00:51<00:31,  8.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 406/659 [00:51<00:31,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 407/659 [00:51<00:31,  8.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 408/659 [00:51<00:30,  8.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 409/659 [00:51<00:30,  8.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 410/659 [00:51<00:30,  8.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 411/659 [00:52<00:31,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|██████▎   | 412/659 [00:52<00:31,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|██████▎   | 413/659 [00:52<00:31,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|██████▎   | 414/659 [00:52<00:30,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|██████▎   | 415/659 [00:52<00:30,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|██████▎   | 416/659 [00:52<00:30,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|██████▎   | 417/659 [00:52<00:29,  8.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|██████▎   | 418/659 [00:52<00:29,  8.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▎   | 419/659 [00:53<00:30,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▎   | 420/659 [00:53<00:30,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 421/659 [00:53<00:29,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 422/659 [00:53<00:29,  8.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 423/659 [00:53<00:28,  8.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 424/659 [00:53<00:28,  8.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 425/659 [00:53<00:29,  8.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|██████▍   | 426/659 [00:53<00:29,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|██████▍   | 427/659 [00:54<00:29,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|██████▍   | 428/659 [00:54<00:29,  7.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|██████▌   | 429/659 [00:54<00:29,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|██████▌   | 430/659 [00:54<00:29,  7.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|██████▌   | 431/659 [00:54<00:29,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 432/659 [00:54<00:29,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 433/659 [00:54<00:29,  7.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 434/659 [00:54<00:28,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 435/659 [00:55<00:28,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 436/659 [00:55<00:28,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▋   | 437/659 [00:55<00:28,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▋   | 438/659 [00:55<00:27,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 439/659 [00:55<00:27,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 440/659 [00:55<00:27,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 441/659 [00:55<00:27,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 442/659 [00:55<00:27,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 443/659 [00:56<00:27,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 444/659 [00:56<00:28,  7.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 445/659 [00:56<00:27,  7.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 446/659 [00:56<00:27,  7.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 447/659 [00:56<00:27,  7.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 448/659 [00:56<00:27,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 449/659 [00:56<00:26,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 450/659 [00:56<00:26,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 451/659 [00:57<00:26,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|██████▊   | 452/659 [00:57<00:25,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|██████▊   | 453/659 [00:57<00:25,  8.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|██████▉   | 454/659 [00:57<00:25,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|██████▉   | 455/659 [00:57<00:25,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|██████▉   | 456/659 [00:57<00:25,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|██████▉   | 457/659 [00:57<00:25,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|██████▉   | 458/659 [00:57<00:25,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|██████▉   | 459/659 [00:58<00:26,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|██████▉   | 460/659 [00:58<00:25,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|██████▉   | 461/659 [00:58<00:25,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 462/659 [00:58<00:25,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 463/659 [00:58<00:24,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 464/659 [00:58<00:24,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████   | 465/659 [00:58<00:24,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████   | 466/659 [00:58<00:23,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████   | 467/659 [00:59<00:24,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████   | 468/659 [00:59<00:24,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████   | 469/659 [00:59<00:24,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████▏  | 470/659 [00:59<00:23,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████▏  | 471/659 [00:59<00:23,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 472/659 [00:59<00:23,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 473/659 [00:59<00:23,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 474/659 [00:59<00:23,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 475/659 [01:00<00:23,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 476/659 [01:00<00:23,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 477/659 [01:00<00:23,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|███████▎  | 478/659 [01:00<00:23,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|███████▎  | 479/659 [01:00<00:22,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|███████▎  | 480/659 [01:00<00:22,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|███████▎  | 481/659 [01:00<00:22,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|███████▎  | 482/659 [01:01<00:22,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|███████▎  | 483/659 [01:01<00:22,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|███████▎  | 484/659 [01:01<00:21,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▎  | 485/659 [01:01<00:21,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▎  | 486/659 [01:01<00:21,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 487/659 [01:01<00:21,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 488/659 [01:01<00:21,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 489/659 [01:01<00:21,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 490/659 [01:02<00:21,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▍  | 491/659 [01:02<00:21,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▍  | 492/659 [01:02<00:21,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▍  | 493/659 [01:02<00:20,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▍  | 494/659 [01:02<00:20,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▌  | 495/659 [01:02<00:20,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▌  | 496/659 [01:02<00:20,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▌  | 497/659 [01:02<00:20,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 498/659 [01:03<00:20,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 499/659 [01:03<00:20,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 500/659 [01:03<00:20,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 501/659 [01:03<00:20,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 502/659 [01:03<00:19,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▋  | 503/659 [01:03<00:20,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▋  | 504/659 [01:03<00:20,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|███████▋  | 505/659 [01:03<00:19,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|███████▋  | 506/659 [01:04<00:19,  7.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|███████▋  | 507/659 [01:04<00:19,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|███████▋  | 508/659 [01:04<00:19,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|███████▋  | 509/659 [01:04<00:19,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|███████▋  | 510/659 [01:04<00:18,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 511/659 [01:04<00:18,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 512/659 [01:04<00:18,  8.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 513/659 [01:04<00:17,  8.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 514/659 [01:05<00:17,  8.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 515/659 [01:05<00:17,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 516/659 [01:05<00:18,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 517/659 [01:05<00:18,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▊  | 518/659 [01:05<00:17,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▉  | 519/659 [01:05<00:17,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▉  | 520/659 [01:05<00:17,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▉  | 521/659 [01:05<00:17,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▉  | 522/659 [01:06<00:17,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▉  | 523/659 [01:06<00:16,  8.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|███████▉  | 524/659 [01:06<00:16,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|███████▉  | 525/659 [01:06<00:16,  8.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|███████▉  | 526/659 [01:06<00:16,  8.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|███████▉  | 527/659 [01:06<00:16,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 528/659 [01:06<00:16,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 529/659 [01:06<00:16,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 530/659 [01:07<00:16,  7.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|████████  | 531/659 [01:07<00:16,  7.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|████████  | 532/659 [01:07<00:16,  7.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|████████  | 533/659 [01:07<00:16,  7.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|████████  | 534/659 [01:07<00:15,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|████████  | 535/659 [01:07<00:15,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|████████▏ | 536/659 [01:07<00:15,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|████████▏ | 537/659 [01:07<00:15,  7.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 538/659 [01:08<00:15,  7.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 539/659 [01:08<00:15,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 540/659 [01:08<00:15,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 541/659 [01:08<00:14,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 542/659 [01:08<00:14,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 543/659 [01:08<00:14,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 544/659 [01:08<00:14,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 545/659 [01:09<00:14,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 546/659 [01:09<00:14,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 547/659 [01:09<00:14,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 548/659 [01:09<00:14,  7.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 549/659 [01:09<00:14,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 550/659 [01:09<00:13,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▎ | 551/659 [01:09<00:13,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 552/659 [01:09<00:13,  8.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 553/659 [01:10<00:12,  8.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 554/659 [01:10<00:13,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 555/659 [01:10<00:13,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 556/659 [01:10<00:13,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|████████▍ | 557/659 [01:10<00:13,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|████████▍ | 558/659 [01:10<00:12,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|████████▍ | 559/659 [01:10<00:12,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|████████▍ | 560/659 [01:10<00:12,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|████████▌ | 561/659 [01:11<00:12,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|████████▌ | 562/659 [01:11<00:12,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|████████▌ | 563/659 [01:11<00:12,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 564/659 [01:11<00:11,  8.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 565/659 [01:11<00:11,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 566/659 [01:11<00:11,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 567/659 [01:11<00:11,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 568/659 [01:11<00:11,  7.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▋ | 569/659 [01:12<00:11,  7.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▋ | 570/659 [01:12<00:11,  7.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 87%|████████▋ | 571/659 [01:12<00:11,  7.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 87%|████████▋ | 572/659 [01:12<00:10,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 87%|████████▋ | 573/659 [01:12<00:10,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 87%|████████▋ | 574/659 [01:12<00:10,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 87%|████████▋ | 575/659 [01:12<00:10,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 87%|████████▋ | 576/659 [01:12<00:10,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 577/659 [01:13<00:10,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 578/659 [01:13<00:10,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 579/659 [01:13<00:10,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 580/659 [01:13<00:10,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 581/659 [01:13<00:09,  7.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 582/659 [01:13<00:09,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 583/659 [01:13<00:09,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 89%|████████▊ | 584/659 [01:13<00:09,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 89%|████████▉ | 585/659 [01:14<00:09,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 89%|████████▉ | 586/659 [01:14<00:08,  8.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 89%|████████▉ | 587/659 [01:14<00:08,  8.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 89%|████████▉ | 588/659 [01:14<00:08,  8.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 89%|████████▉ | 589/659 [01:14<00:08,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|████████▉ | 590/659 [01:14<00:08,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|████████▉ | 591/659 [01:14<00:08,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|████████▉ | 592/659 [01:14<00:08,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|████████▉ | 593/659 [01:15<00:08,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 594/659 [01:15<00:08,  7.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 595/659 [01:15<00:08,  7.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 596/659 [01:15<00:08,  7.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 91%|█████████ | 597/659 [01:15<00:08,  7.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 91%|█████████ | 598/659 [01:15<00:07,  7.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 91%|█████████ | 599/659 [01:15<00:07,  7.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 91%|█████████ | 600/659 [01:15<00:07,  7.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 91%|█████████ | 601/659 [01:16<00:07,  7.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 91%|█████████▏| 602/659 [01:16<00:07,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 603/659 [01:16<00:07,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 604/659 [01:16<00:07,  7.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 605/659 [01:16<00:06,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 606/659 [01:16<00:06,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 607/659 [01:16<00:06,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 608/659 [01:17<00:06,  7.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 609/659 [01:17<00:06,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|█████████▎| 610/659 [01:17<00:06,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|█████████▎| 611/659 [01:17<00:06,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|█████████▎| 612/659 [01:17<00:05,  8.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|█████████▎| 613/659 [01:17<00:05,  7.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|█████████▎| 614/659 [01:17<00:05,  7.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|█████████▎| 615/659 [01:17<00:05,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|█████████▎| 616/659 [01:18<00:05,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▎| 617/659 [01:18<00:05,  8.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 618/659 [01:18<00:05,  8.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 619/659 [01:18<00:04,  8.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 620/659 [01:18<00:04,  8.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 621/659 [01:18<00:04,  8.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 622/659 [01:18<00:04,  8.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|█████████▍| 623/659 [01:18<00:04,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|█████████▍| 624/659 [01:19<00:04,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|█████████▍| 625/659 [01:19<00:04,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|█████████▍| 626/659 [01:19<00:04,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|█████████▌| 627/659 [01:19<00:03,  8.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|█████████▌| 628/659 [01:19<00:03,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|█████████▌| 629/659 [01:19<00:03,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 630/659 [01:19<00:03,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 631/659 [01:19<00:03,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 632/659 [01:20<00:03,  7.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 633/659 [01:20<00:03,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 634/659 [01:20<00:03,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▋| 635/659 [01:20<00:02,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 636/659 [01:20<00:02,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 637/659 [01:20<00:02,  7.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 638/659 [01:20<00:02,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 639/659 [01:20<00:02,  7.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 640/659 [01:21<00:02,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 641/659 [01:21<00:02,  7.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 642/659 [01:21<00:02,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 643/659 [01:21<00:02,  7.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 644/659 [01:21<00:01,  8.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 645/659 [01:21<00:01,  8.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 646/659 [01:21<00:01,  8.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 647/659 [01:21<00:01,  8.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 648/659 [01:22<00:01,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 649/659 [01:22<00:01,  8.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 99%|█████████▊| 650/659 [01:22<00:01,  8.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 99%|█████████▉| 651/659 [01:22<00:00,  8.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 99%|█████████▉| 652/659 [01:22<00:00,  8.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 99%|█████████▉| 653/659 [01:22<00:00,  8.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 99%|█████████▉| 654/659 [01:22<00:00,  7.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 99%|█████████▉| 655/659 [01:22<00:00,  7.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|█████████▉| 656/659 [01:23<00:00,  7.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|█████████▉| 657/659 [01:23<00:00,  7.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|█████████▉| 658/659 [01:23<00:00,  8.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 659/659 [01:23<00:00,  7.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aPsgjgP8oDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_df.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViLIi3lp7PLA",
        "colab_type": "code",
        "outputId": "7907f7c7-5442-4643-fb0d-78cbc73c4afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "predicted_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>rec_items</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[1172, 1293, 1405, 3671, 1339]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>[589, 296, 588, 356, 10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>[318, 6377, 356, 7153, 1196]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>[1200, 590, 1136, 1291, 364]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>[2355, 356, 1580, 364, 377]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>666</td>\n",
              "      <td>[356, 590, 344, 150, 480]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>667</td>\n",
              "      <td>[296, 161, 480, 357, 597]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>669</td>\n",
              "      <td>[785, 2710, 2396, 2599, 2724]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>670</td>\n",
              "      <td>[318, 296, 32, 150, 110]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>671</td>\n",
              "      <td>[4886, 318, 1197, 2797, 2716]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>659 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     userId                       rec_items\n",
              "0         1  [1172, 1293, 1405, 3671, 1339]\n",
              "1         2        [589, 296, 588, 356, 10]\n",
              "2         3    [318, 6377, 356, 7153, 1196]\n",
              "3         4    [1200, 590, 1136, 1291, 364]\n",
              "4         5     [2355, 356, 1580, 364, 377]\n",
              "..      ...                             ...\n",
              "654     666       [356, 590, 344, 150, 480]\n",
              "655     667       [296, 161, 480, 357, 597]\n",
              "656     669   [785, 2710, 2396, 2599, 2724]\n",
              "657     670        [318, 296, 32, 150, 110]\n",
              "658     671   [4886, 318, 1197, 2797, 2716]\n",
              "\n",
              "[659 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQl6epV18XaJ",
        "colab_type": "code",
        "outputId": "e1a6fd1c-587d-4a37-89c1-711557e720ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "train_df.query"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>itemId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>84923</th>\n",
              "      <td>570</td>\n",
              "      <td>1258</td>\n",
              "      <td>1</td>\n",
              "      <td>2016-10-06 20:05:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10930</th>\n",
              "      <td>73</td>\n",
              "      <td>4734</td>\n",
              "      <td>1</td>\n",
              "      <td>2009-10-15 07:25:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32579</th>\n",
              "      <td>235</td>\n",
              "      <td>3623</td>\n",
              "      <td>1</td>\n",
              "      <td>2005-03-23 06:05:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49454</th>\n",
              "      <td>361</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1997-05-19 14:13:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56531</th>\n",
              "      <td>407</td>\n",
              "      <td>3174</td>\n",
              "      <td>1</td>\n",
              "      <td>2000-07-03 18:01:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17422</th>\n",
              "      <td>115</td>\n",
              "      <td>2706</td>\n",
              "      <td>1</td>\n",
              "      <td>2005-01-29 21:34:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49694</th>\n",
              "      <td>363</td>\n",
              "      <td>1037</td>\n",
              "      <td>1</td>\n",
              "      <td>1999-11-11 19:05:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65956</th>\n",
              "      <td>468</td>\n",
              "      <td>1394</td>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-28 06:34:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50994</th>\n",
              "      <td>378</td>\n",
              "      <td>6870</td>\n",
              "      <td>1</td>\n",
              "      <td>2015-09-26 18:32:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31717</th>\n",
              "      <td>232</td>\n",
              "      <td>531</td>\n",
              "      <td>1</td>\n",
              "      <td>2000-04-07 06:29:58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70002 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       userId  itemId  rating           timestamp\n",
              "84923     570    1258       1 2016-10-06 20:05:06\n",
              "10930      73    4734       1 2009-10-15 07:25:38\n",
              "32579     235    3623       1 2005-03-23 06:05:15\n",
              "49454     361       7       1 1997-05-19 14:13:17\n",
              "56531     407    3174       1 2000-07-03 18:01:07\n",
              "...       ...     ...     ...                 ...\n",
              "17422     115    2706       1 2005-01-29 21:34:31\n",
              "49694     363    1037       1 1999-11-11 19:05:19\n",
              "65956     468    1394       1 2011-01-28 06:34:59\n",
              "50994     378    6870       1 2015-09-26 18:32:21\n",
              "31717     232     531       1 2000-04-07 06:29:58\n",
              "\n",
              "[70002 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RCtrpHm7ZGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ground_truth_df = collect_ground_truth(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VepxPbs7YJ1",
        "colab_type": "code",
        "outputId": "df08c73f-181f-4702-d11c-ba2e775d03d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "k=5\n",
        "for metric_func in [precision_at_k, recall_at_k, hit_rate_at_k, ndcg_at_k, average_precision_at_k]:\n",
        "  print('{:s}: {:f}'.format(metric_func.__name__, mean_metric(predicted_df, ground_truth_df,metric_func,k=k)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision_at_k: 0.014568\n",
            "recall_at_k: 0.011896\n",
            "hit_rate_at_k: 0.072838\n",
            "ndcg_at_k: 0.010221\n",
            "average_precision_at_k: 0.026505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JprvR7EJBezc",
        "colab_type": "text"
      },
      "source": [
        "# implicit ALS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_WB0HsjSWVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ALSRecommender():\n",
        "  def __init__(self, factors):\n",
        "    # dicts to store id <-> idx mappings\n",
        "    self.user_to_idx = None\n",
        "    self.idx_to_user = None\n",
        "    self.item_to_idx = None\n",
        "    self.idx_to_item = None\n",
        "\n",
        "    self.factors = factors\n",
        "    # initialize model\n",
        "    self.model = implicit.als.AlternatingLeastSquares(factors=self.factors)\n",
        "\n",
        "    self.train_users = None\n",
        "\n",
        "  def fit(self, X):\n",
        "    \"\"\"Build user-item matrix and train ALS\n",
        "\n",
        "    :param X: incoming data for fitting contains 'userId', 'itemId' and 'rating' columns (DataFrame)\n",
        "    \"\"\"\n",
        "    # create copy to operate with\n",
        "    X_copy = deepcopy(X)\n",
        "    # this model need to store train users for correct prediction\n",
        "    self.train_users = X_copy['userId'].unique()\n",
        "    # create mappings\n",
        "    self.user_to_idx = {u:idx for idx, u in enumerate(sorted(X_copy['userId'].unique()))}\n",
        "    self.idx_to_user = {idx:u for u, idx in self.user_to_idx.items()}\n",
        "    self.item_to_idx = {i:idx for idx, i in enumerate(sorted(X_copy['itemId'].unique()))}\n",
        "    self.idx_to_item = {idx:i for i, idx in self.item_to_idx.items()}\n",
        "    \n",
        "    # map id on idx\n",
        "    X_copy['userIdx'] = X_copy['userId'].apply(lambda x: self.user_to_idx[x])\n",
        "    X_copy['itemIdx'] = X_copy['itemId'].apply(lambda x: self.item_to_idx[x])\n",
        "    \n",
        "    # build item-user matrix\n",
        "    self.item_user_matrix = csr_matrix(X_copy.pivot(index='itemIdx',\n",
        "                               columns='userIdx',\n",
        "                               values='rating'\n",
        "                              ).fillna(0).values)\n",
        "    # fit model\n",
        "    self.model.fit(self.item_user_matrix)\n",
        "    \n",
        "  def predict(self, X, top_n):\n",
        "    \"\"\"Predict top_n popular items for each user\n",
        "    \n",
        "    :param X: incoming data for prediction contains 'userId' column (DataFrame)\n",
        "    :param top_n: number of recommendations (int)\n",
        "    :return: items recommendations for each user (DataFrame)\n",
        "    \"\"\"\n",
        "    predicted_df = pd.DataFrame()\n",
        "    # prediction can be made only for users who are presented into train and test both\n",
        "    predicted_df['userId'] = list(set(X['userId'].unique()) & set(self.train_users))\n",
        "    # initialize column with nulls\n",
        "    predicted_df['rec_items'] = 0\n",
        "    # some tricks to assign lists to dataframe cells\n",
        "    predicted_df.set_index('userId', inplace=True)\n",
        "    predicted_df = predicted_df.astype('object')\n",
        "    # required by ALS interface\n",
        "    user_item_matrix = self.item_user_matrix.T.tocsr()\n",
        "\n",
        "    for user in tqdm(X['userId'].unique()):\n",
        "      if user in self.train_users:\n",
        "        recommendations = self.model.recommend(self.user_to_idx[user], user_item_matrix, filter_already_liked_items=True, N=top_n)\n",
        "        best_items = [self.idx_to_item[item] for (item, score) in recommendations]\n",
        "        # use at to assign list\n",
        "        predicted_df.at[user, 'rec_items'] = best_items\n",
        "    predicted_df.reset_index(inplace=True)\n",
        "    return predicted_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYRihYqv4qp",
        "colab_type": "text"
      },
      "source": [
        "# BPR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9me0VS0qTuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BPRRecommender():\n",
        "  def __init__(self, factors):\n",
        "    # dicts to store id <-> idx mappings\n",
        "    self.user_to_idx = None\n",
        "    self.idx_to_user = None\n",
        "    self.item_to_idx = None\n",
        "    self.idx_to_item = None\n",
        "\n",
        "    self.factors = factors\n",
        "    # initialize model\n",
        "    # it is impossible to use gpu version due to bug in library\n",
        "    self.model = implicit.bpr.BayesianPersonalizedRanking(factors=self.factors, use_gpu=False, verify_negative_samples=True)\n",
        "\n",
        "    self.train_users = None\n",
        "\n",
        "  def fit(self, X):\n",
        "    \"\"\"Build user-item matrix and train BPR\n",
        "\n",
        "    :param X: incoming data for fitting contains 'userId', 'itemId' and 'rating' columns (DataFrame)\n",
        "    \"\"\"\n",
        "    # create copy to operate with\n",
        "    X_copy = deepcopy(X)\n",
        "    # this model need to store train users for correct prediction\n",
        "    self.train_users = X_copy['userId'].unique()\n",
        "    # create mappings\n",
        "    self.user_to_idx = {u:idx for idx, u in enumerate(sorted(X_copy['userId'].unique()))}\n",
        "    self.idx_to_user = {idx:u for u, idx in self.user_to_idx.items()}\n",
        "    self.item_to_idx = {i:idx for idx, i in enumerate(sorted(X_copy['itemId'].unique()))}\n",
        "    self.idx_to_item = {idx:i for i, idx in self.item_to_idx.items()}\n",
        "    \n",
        "    # map id on idx\n",
        "    X_copy['userIdx'] = X_copy['userId'].apply(lambda x: self.user_to_idx[x])\n",
        "    X_copy['itemIdx'] = X_copy['itemId'].apply(lambda x: self.item_to_idx[x])\n",
        "    \n",
        "    # build item-user matrix\n",
        "    self.item_user_matrix = coo_matrix(X_copy.pivot(index='itemIdx',\n",
        "                               columns='userIdx',\n",
        "                               values='rating'\n",
        "                              ).fillna(0).values)\n",
        "    # fit model\n",
        "    self.model.fit(self.item_user_matrix)\n",
        "    \n",
        "  def predict(self, X, top_n):\n",
        "    \"\"\"Predict top_n popular items for each user\n",
        "    \n",
        "    :param X: incoming data for prediction contains 'userId' column (DataFrame)\n",
        "    :param top_n: number of recommendations (int)\n",
        "    :return: items recommendations for each user (DataFrame)\n",
        "    \"\"\"\n",
        "    predicted_df = pd.DataFrame()\n",
        "    # prediction can be made only for users who are presented into train and test both\n",
        "    predicted_df['userId'] = list(set(X['userId'].unique()) & set(self.train_users))\n",
        "    # initialize column with nulls\n",
        "    predicted_df['rec_items'] = 0\n",
        "    # some tricks to assign lists to dataframe cells\n",
        "    predicted_df.set_index('userId', inplace=True)\n",
        "    predicted_df = predicted_df.astype('object')\n",
        "    # required by ALS interface\n",
        "    user_item_matrix = self.item_user_matrix.T.tocsr()\n",
        "\n",
        "    for user in tqdm(X['userId'].unique()):\n",
        "      if user in self.train_users:\n",
        "        recommendations = self.model.recommend(self.user_to_idx[user], user_item_matrix, filter_already_liked_items=True, N=top_n)\n",
        "        best_items = [self.idx_to_item[item] for (item, score) in recommendations]\n",
        "        # use at to assign list\n",
        "        predicted_df.at[user, 'rec_items'] = best_items\n",
        "    predicted_df.reset_index(inplace=True)\n",
        "    return predicted_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0_EZzHY-Gbj",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDhy-9bn-JjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_ground_truth(X):\n",
        "  \"\"\"Build list of ground truth for every user based on X provided data\"\"\"\n",
        "  \n",
        "  return X.groupby('userId').agg({'itemId':list}).reset_index().rename(columns={'itemId':'ground_truth_items'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQwrmAE8JiTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_at_k(predicted, ground_truth, k):\n",
        "  assert len(predicted) >= k\n",
        "  intersection = set(predicted[:k]) & set(ground_truth)\n",
        "  return len(intersection) / k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMFBuzPkKs42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall_at_k(predicted, ground_truth, k):\n",
        "  assert len(predicted) >= k\n",
        "  intersection = set(predicted[:k]) & set(ground_truth)\n",
        "  return len(intersection) / len(ground_truth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbL9QYpZv6Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hit_rate_at_k(predicted, ground_truth, k):\n",
        "  assert len(predicted) == k\n",
        "  intersection = set(predicted[:k]) & set(ground_truth)\n",
        "  return 1 if len(intersection) > 0 else 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOc9NXCJXYjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ndcg_at_k(predicted, ground_truth, k):\n",
        "  assert len(predicted) >= k\n",
        "  gain = sum([1/np.log2(i+2) for i, item in enumerate(predicted[:k]) if item in ground_truth])\n",
        "  max_gain = sum([1/np.log2(i+2) for i in range(len(ground_truth))])\n",
        "  return gain / max_gain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fjNLPdB5fpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_precision_at_k(predicted, actual, k):\n",
        "  tmp = 0\n",
        "  actual_length = min(k,len(actual))\n",
        "  if len(predicted) < actual_length:\n",
        "    warnings.warn(\"Length of predict is less than k\")\n",
        "  for i in range(actual_length):\n",
        "    if predicted[i] in actual:\n",
        "      tmp += precision_at_k(predicted[:i+1], actual, i+1)\n",
        "  return tmp / actual_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihObnV19bywG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_precision_at_k(predicted, ground_truth, k):\n",
        "  if len(set(predicted) & set(ground_truth)) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.mean([precision_at_k(predicted[:i+1], ground_truth, i+1) for i in range(k) if predicted[i] in ground_truth])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhVuf9vhb7GH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_metric(predicted_df, ground_truth_df, metric_func, k):\n",
        "  \"\"\"Calculate averaged by users score for passed metric\"\"\"\n",
        "  merged_df = pd.merge(ground_truth_df, predicted_df, on='userId')\n",
        "  return np.mean([metric_func(row['rec_items'], row['ground_truth_items'], k=k) for _, row in  merged_df.iterrows()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syvZyqc0JoxQ",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39a0FdsiJ35f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7XVDYbUKFMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ground_truth_df = collect_ground_truth(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWeTHyfjJqrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mp_rec = MostPopularRecommender()\n",
        "mp_rec.fit(train_df)\n",
        "predicted_df = mp_rec.predict(test_df, top_n=k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H49w3wbIKEA0",
        "colab_type": "code",
        "outputId": "149bc56a-5027-4366-82b4-4170c68355be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for metric_func in [precision_at_k, recall_at_k, hit_rate_at_k, ndcg_at_k, average_precision_at_k]:\n",
        "  print('{:s}: {:f}'.format(metric_func.__name__, mean_metric(predicted_df, ground_truth_df,metric_func,k=k)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision_at_k: 0.046483\n",
            "recall_at_k: 0.030537\n",
            "hit_rate_at_k: 0.197248\n",
            "ndcg_at_k: 0.032528\n",
            "average_precision_at_k: 0.098620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni2FCFBvYBXq",
        "colab_type": "code",
        "outputId": "695016c5-d1a7-4751-e900-ee92ff975cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "55d4f3f134d045adba8150b25a44a325",
            "941ff7770b5940cb96e12c0435181596",
            "0c91bc7454104b4abdf3bb96c0ff2394",
            "9b6c9f3806ca4a9e8356540c5840aff9",
            "f2d1b23e0c5f416db14bed650e0b5f30",
            "24510eac06374e79af27ee4dd0b2567b",
            "9a62daf19d3b4d3aa1eea9734159aa63",
            "e5c9a804b9cc483c805548710eff2097"
          ]
        }
      },
      "source": [
        "als_rec = ALSRecommender(30)\n",
        "als_rec.fit(train_df)\n",
        "predicted_df = als_rec.predict(test_df, top_n=k)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:implicit:GPU training requires factor size to be a multiple of 32. Increasing factors from 30 to 32.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55d4f3f134d045adba8150b25a44a325",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 50/654 [00:00<00:01, 496.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 654/654 [00:00<00:00, 1118.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYWVt5F_oeEP",
        "colab_type": "code",
        "outputId": "7a956e2a-ebc6-4cbc-94fb-88bbaeb9b94a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for metric_func in [precision_at_k, recall_at_k, hit_rate_at_k, ndcg_at_k, average_precision_at_k]:\n",
        "  print('{:s}: {:f}'.format(metric_func.__name__, mean_metric(predicted_df, ground_truth_df,metric_func,k=k)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision_at_k: 0.136391\n",
            "recall_at_k: 0.087220\n",
            "hit_rate_at_k: 0.466361\n",
            "ndcg_at_k: 0.097139\n",
            "average_precision_at_k: 0.251504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aSXgmDjqqIm",
        "colab_type": "code",
        "outputId": "ba2caebc-0619-4fbd-d824-0a88c385f383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "3e0631cee3974e4bb9498cb981976394",
            "e6648b88785a44ad9dc0106b400f53ed",
            "b5a5db44996c43ba87301753210b9edf",
            "55b46316404d409eac4579d8fa26f165",
            "24d9e7a7f11f4e6e9fc5998894374cb7",
            "21a7f82ad6094707b671b68947c5b38d",
            "1ac89d4ef378492886aac25ee0815c69",
            "a83eb6973fff42ac82a8cb7339431224"
          ]
        }
      },
      "source": [
        "bpr_rec = BPRRecommender(30)\n",
        "bpr_rec.fit(train_df)\n",
        "predicted_df = bpr_rec.predict(test_df, top_n=k)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e0631cee3974e4bb9498cb981976394",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|▉         | 64/654 [00:00<00:00, 635.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 654/654 [00:00<00:00, 1152.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkUtw-VsuGdU",
        "colab_type": "code",
        "outputId": "17906645-3d42-4dc6-ecf7-9d2e2a1bcd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for metric_func in [precision_at_k, recall_at_k, hit_rate_at_k, ndcg_at_k, average_precision_at_k]:\n",
        "  print('{:s}: {:f}'.format(metric_func.__name__, mean_metric(predicted_df, ground_truth_df,metric_func,k=k)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision_at_k: 0.096636\n",
            "recall_at_k: 0.059776\n",
            "hit_rate_at_k: 0.373089\n",
            "ndcg_at_k: 0.071401\n",
            "average_precision_at_k: 0.210555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIuqhdTDSMDi",
        "colab_type": "text"
      },
      "source": [
        "# NCF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "graEw0QBOlzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8pDHoVT9kt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = deepcopy(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DluZuyT0D2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_to_idx = {u:idx for idx, u in enumerate(sorted(X['userId'].unique()))}\n",
        "idx_to_user = {idx:u for u, idx in user_to_idx.items()}\n",
        "item_to_idx = {i:idx for idx, i in enumerate(sorted(X['itemId'].unique()))}\n",
        "idx_to_item = {idx:i for i, idx in item_to_idx.items()}\n",
        "\n",
        "X['userIdx'] = X['userId'].apply(lambda x: user_to_idx[x])\n",
        "X['itemIdx'] = X['itemId'].apply(lambda x: item_to_idx[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQMNMbVhUQfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UserItemRatingDataset(Dataset):\n",
        "    \"\"\"Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset\"\"\"\n",
        "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair\n",
        "        \"\"\"\n",
        "        self.user_tensor = user_tensor\n",
        "        self.item_tensor = item_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.user_tensor.size(0)\n",
        "\n",
        "\n",
        "class SampleGenerator(object):\n",
        "    \"\"\"Construct dataset for NCF\"\"\"\n",
        "\n",
        "    def __init__(self, ratings):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            ratings: pd.DataFrame, which contains 4 columns = ['userIdx', 'itemIdx', 'rating', 'timestamp']\n",
        "        \"\"\"\n",
        "        assert 'userIdx' in ratings.columns\n",
        "        assert 'itemIdx' in ratings.columns\n",
        "        assert 'rating' in ratings.columns\n",
        "\n",
        "        self.ratings = ratings\n",
        "        # explicit feedback using _normalize and implicit using _binarize\n",
        "        # self.preprocess_ratings = self._normalize(ratings)\n",
        "        self.preprocess_ratings = self._binarize(ratings)\n",
        "        self.user_pool = set(self.ratings['userIdx'].unique())\n",
        "        self.item_pool = set(self.ratings['itemIdx'].unique())\n",
        "        # create negative item samples for NCF learning\n",
        "        self.negatives = self._sample_negative(ratings)\n",
        "        self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings)\n",
        "\n",
        "    def _normalize(self, ratings):\n",
        "        \"\"\"normalize into [0, 1] from [0, max_rating], explicit feedback\"\"\"\n",
        "        ratings = deepcopy(ratings)\n",
        "        max_rating = ratings.rating.max()\n",
        "        ratings['rating'] = ratings.rating * 1.0 / max_rating\n",
        "        return ratings\n",
        "    \n",
        "    def _binarize(self, ratings):\n",
        "        \"\"\"binarize into 0 or 1, imlicit feedback\"\"\"\n",
        "        ratings = deepcopy(ratings)\n",
        "        ratings['rating'][ratings['rating'] > 0] = 1.0\n",
        "        return ratings\n",
        "\n",
        "    def _split_loo(self, ratings):\n",
        "        \"\"\"leave one out train/test split \"\"\"\n",
        "        ratings['rank_latest'] = ratings.groupby(['userIdx'])['timestamp'].rank(method='first', ascending=False)\n",
        "        test = ratings[ratings['rank_latest'] == 1]\n",
        "        train = ratings[ratings['rank_latest'] > 1]\n",
        "        assert train['userIdx'].nunique() == test['userIdx'].nunique()\n",
        "        return train[['userIdx', 'itemIdx', 'rating']], test[['userIdx', 'itemIdx', 'rating']]\n",
        "\n",
        "    def _sample_negative(self, ratings):\n",
        "        \"\"\"return all negative items & 100 sampled negative items\"\"\"\n",
        "        interact_status = ratings.groupby('userIdx')['itemIdx'].apply(set).reset_index().rename(\n",
        "            columns={'itemIdx': 'interacted_items'})\n",
        "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
        "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
        "        return interact_status[['userIdx', 'negative_items', 'negative_samples']]\n",
        "\n",
        "    def instance_a_train_loader(self, num_negatives, batch_size):\n",
        "        \"\"\"instance train loader for one training epoch\"\"\"\n",
        "        users, items, ratings = [], [], []\n",
        "        train_ratings = pd.merge(self.train_ratings, self.negatives[['userIdx', 'negative_items']], on='userIdx')\n",
        "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
        "        for row in train_ratings.itertuples():\n",
        "            users.append(int(row.userIdx))\n",
        "            items.append(int(row.itemIdx))\n",
        "            ratings.append(float(row.rating))\n",
        "            for i in range(num_negatives):\n",
        "                users.append(int(row.userIdx))\n",
        "                items.append(int(row.negatives[i]))\n",
        "                ratings.append(float(0))  # negative samples get 0 rating\n",
        "        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),\n",
        "                                        item_tensor=torch.LongTensor(items),\n",
        "                                        target_tensor=torch.FloatTensor(ratings))\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    def create_validation_data(self):\n",
        "        \"\"\"create evaluate data\"\"\"\n",
        "        merged_ratings = pd.merge(self.test_ratings, self.negatives[['userIdx', 'negative_samples']], on='userIdx')\n",
        "        valid_users, valid_items, valid_ratings = [], [], []\n",
        "\n",
        "        for row in merged_ratings.itertuples():\n",
        "            valid_users.append(int(row.userIdx))\n",
        "            valid_items.append(int(row.itemIdx))\n",
        "            valid_ratings.append(float(1))\n",
        "            for i in range(len(row.negative_samples)):\n",
        "                valid_users.append(int(row.userIdx))\n",
        "                valid_items.append(int(row.negative_samples[i]))\n",
        "                valid_ratings.append(float(0))\n",
        "        return [torch.LongTensor(valid_users), torch.LongTensor(valid_items), torch.FloatTensor(valid_ratings)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AijDf5MTYMDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Some handy functions for pytroch model training ...\n",
        "\"\"\"\n",
        "# Checkpoints\n",
        "def save_checkpoint(model, model_dir):\n",
        "    torch.save(model.state_dict(), model_dir)\n",
        "\n",
        "\n",
        "def resume_checkpoint(model, model_dir, device_id):\n",
        "    state_dict = torch.load(model_dir,\n",
        "                            map_location=lambda storage, loc: storage.cuda(device=device_id))  # ensure all storage are on gpu\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "# Hyper params\n",
        "def use_cuda(enabled, device_id=0):\n",
        "    if enabled:\n",
        "        assert torch.cuda.is_available(), 'CUDA is not available'\n",
        "        torch.cuda.set_device(device_id)\n",
        "\n",
        "\n",
        "def use_optimizer(network, params):\n",
        "    if params['optimizer'] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(network.parameters(),\n",
        "                                    lr=params['sgd_lr'],\n",
        "                                    momentum=params['sgd_momentum'],\n",
        "                                    weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'adam':\n",
        "        optimizer = torch.optim.Adam(network.parameters(), \n",
        "                                                          lr=params['adam_lr'],\n",
        "                                                          weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(network.parameters(),\n",
        "                                        lr=params['rmsprop_lr'],\n",
        "                                        alpha=params['rmsprop_alpha'],\n",
        "                                        momentum=params['rmsprop_momentum'])\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkte8Fk_YG4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Engine(object):\n",
        "    \"\"\"Meta Engine for training & evaluating NCF model\n",
        "    Note: Subclass should implement self.model !\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config  # model configuration\n",
        "        self.opt = use_optimizer(self.model, config)\n",
        "        # explicit feedback\n",
        "        # self.crit = torch.nn.MSELoss()\n",
        "        # implicit feedback\n",
        "        self.crit = torch.nn.BCELoss()\n",
        "\n",
        "    def train_single_batch(self, users, items, ratings):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        if self.config['use_cuda'] is True:\n",
        "            users, items, ratings = users.cuda(), items.cuda(), ratings.cuda()\n",
        "        self.opt.zero_grad()\n",
        "        ratings_pred = self.model(users, items)\n",
        "        loss = self.crit(ratings_pred.view(-1), ratings)\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "        loss = loss.item()\n",
        "        return loss\n",
        "\n",
        "    def train_an_epoch(self, train_loader, epoch_id):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch_id, batch in enumerate(train_loader):\n",
        "            assert isinstance(batch[0], torch.LongTensor)\n",
        "            user, item, rating = batch[0], batch[1], batch[2]\n",
        "            rating = rating.float()\n",
        "            loss = self.train_single_batch(user, item, rating)\n",
        "            total_loss += loss\n",
        "        print('[Training Epoch {}], Loss {}'.format(epoch_id, total_loss/len(train_loader)))\n",
        "\n",
        "    def evaluate(self, evaluate_data):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_users, valid_items, valid_ratings = evaluate_data[0], evaluate_data[1], evaluate_data[2]\n",
        "            if self.config['use_cuda'] is True:\n",
        "                valid_users = valid_users.cuda()\n",
        "                valid_items = valid_items.cuda()\n",
        "                valid_ratings = valid_ratings.cuda()\n",
        "            valid_scores = self.model(valid_users, valid_items)\n",
        "            loss = self.crit(valid_scores.view(-1), valid_ratings)\n",
        "            return loss.item()\n",
        "            \n",
        "\n",
        "    def save(self, alias, epoch_id):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        model_dir = self.config['model_dir'].format(alias, epoch_id)\n",
        "        save_checkpoint(self.model, model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRHBEw9wX-xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GMF, self).__init__()\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class GMFEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(GMFEngine, self).__init__(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgS_IS2GUfrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gmf_config = {'alias': 'gmf_factor8neg4-implict',\n",
        "              'num_epoch': 80,\n",
        "              'batch_size': 1024,\n",
        "              # 'optimizer': 'sgd',\n",
        "              # 'sgd_lr': 1e-3,\n",
        "              # 'sgd_momentum': 0.9,\n",
        "              # 'optimizer': 'rmsprop',\n",
        "              # 'rmsprop_lr': 1e-3,\n",
        "              # 'rmsprop_alpha': 0.99,\n",
        "              # 'rmsprop_momentum': 0,\n",
        "              'optimizer': 'adam',\n",
        "              'adam_lr': 1e-3,\n",
        "              'num_users': X['userId'].nunique(),\n",
        "              'num_items': X['itemId'].nunique(),\n",
        "              'latent_dim': 8,\n",
        "              'num_negative': 4,\n",
        "              'l2_regularization': 0, # 0.01\n",
        "              'use_cuda': True,\n",
        "              'device_id': 0,\n",
        "              'model_dir':'checkpoints/{}_Epoch{}.model'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHdAMmbFYlKs",
        "colab_type": "code",
        "outputId": "eca8e251-56f0-493d-d8d4-a84ff2cb18bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# DataLoader for training\n",
        "sample_generator = SampleGenerator(ratings=X)\n",
        "validation_data = sample_generator.create_validation_data()\n",
        "config = gmf_config\n",
        "engine = GMFEngine(config)\n",
        "\n",
        "for epoch in range(config['num_epoch']):\n",
        "    print('Epoch {} starts !'.format(epoch))\n",
        "    print('-' * 80)\n",
        "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
        "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
        "    print(engine.evaluate(validation_data))\n",
        "    engine.save(config['alias'], epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 0], Loss 0.6624914193223711\n",
            "0.5497864484786987\n",
            "Epoch 1 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 1], Loss 0.5823789826184951\n",
            "0.4465653896331787\n",
            "Epoch 2 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 2], Loss 0.5429482651671126\n",
            "0.3753364086151123\n",
            "Epoch 3 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 3], Loss 0.5210116267380109\n",
            "0.32575345039367676\n",
            "Epoch 4 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 4], Loss 0.5095029580557944\n",
            "0.29173046350479126\n",
            "Epoch 5 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 5], Loss 0.5039231897631226\n",
            "0.2689037322998047\n",
            "Epoch 6 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 6], Loss 0.5015039296276802\n",
            "0.25433796644210815\n",
            "Epoch 7 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 7], Loss 0.5006374294778942\n",
            "0.24558818340301514\n",
            "Epoch 8 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 8], Loss 0.5002357211612318\n",
            "0.24096958339214325\n",
            "Epoch 9 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 9], Loss 0.4999359888313091\n",
            "0.23847027122974396\n",
            "Epoch 10 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 10], Loss 0.4998104705923075\n",
            "0.23734335601329803\n",
            "Epoch 11 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 11], Loss 0.49939579958409336\n",
            "0.23710881173610687\n",
            "Epoch 12 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 12], Loss 0.4989325091198834\n",
            "0.23777712881565094\n",
            "Epoch 13 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 13], Loss 0.4981740843229941\n",
            "0.23737788200378418\n",
            "Epoch 14 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 14], Loss 0.4968293056902984\n",
            "0.23776857554912567\n",
            "Epoch 15 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 15], Loss 0.4947839032935534\n",
            "0.23778824508190155\n",
            "Epoch 16 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 16], Loss 0.49146652283218406\n",
            "0.23693187534809113\n",
            "Epoch 17 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 17], Loss 0.48668769292071856\n",
            "0.23672091960906982\n",
            "Epoch 18 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 18], Loss 0.4795524967631056\n",
            "0.2353745698928833\n",
            "Epoch 19 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 19], Loss 0.4695154741450397\n",
            "0.2359890341758728\n",
            "Epoch 20 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 20], Loss 0.4568966605142858\n",
            "0.2326028048992157\n",
            "Epoch 21 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 21], Loss 0.44318100555104845\n",
            "0.22872529923915863\n",
            "Epoch 22 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 22], Loss 0.42832367689208645\n",
            "0.2249305248260498\n",
            "Epoch 23 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 23], Loss 0.41454316565772426\n",
            "0.2201717495918274\n",
            "Epoch 24 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 24], Loss 0.400767224728182\n",
            "0.21851229667663574\n",
            "Epoch 25 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 25], Loss 0.38931860122005496\n",
            "0.21283073723316193\n",
            "Epoch 26 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 26], Loss 0.37945446354449675\n",
            "0.2115797996520996\n",
            "Epoch 27 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 27], Loss 0.3708780052739259\n",
            "0.20888835191726685\n",
            "Epoch 28 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 28], Loss 0.3641529777760351\n",
            "0.20479470491409302\n",
            "Epoch 29 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 29], Loss 0.3565935021304803\n",
            "0.2024036943912506\n",
            "Epoch 30 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 30], Loss 0.35107360397819926\n",
            "0.2000533491373062\n",
            "Epoch 31 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 31], Loss 0.3454556893282584\n",
            "0.20033320784568787\n",
            "Epoch 32 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 32], Loss 0.3414962339190255\n",
            "0.19726505875587463\n",
            "Epoch 33 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 33], Loss 0.33679102717247683\n",
            "0.19512593746185303\n",
            "Epoch 34 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 34], Loss 0.33317157476700865\n",
            "0.19548818469047546\n",
            "Epoch 35 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 35], Loss 0.3296643990506816\n",
            "0.19235458970069885\n",
            "Epoch 36 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 36], Loss 0.3266894538669811\n",
            "0.19051377475261688\n",
            "Epoch 37 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 37], Loss 0.3243649623914454\n",
            "0.1898665428161621\n",
            "Epoch 38 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 38], Loss 0.32120196297105436\n",
            "0.18876785039901733\n",
            "Epoch 39 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 39], Loss 0.3182061013922227\n",
            "0.18753960728645325\n",
            "Epoch 40 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 40], Loss 0.3148941647338305\n",
            "0.1872316151857376\n",
            "Epoch 41 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 41], Loss 0.31176238724615724\n",
            "0.18661144375801086\n",
            "Epoch 42 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 42], Loss 0.3098045890837644\n",
            "0.1855996698141098\n",
            "Epoch 43 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 43], Loss 0.3077408512257545\n",
            "0.18415531516075134\n",
            "Epoch 44 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 44], Loss 0.30469400156564064\n",
            "0.18459971249103546\n",
            "Epoch 45 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 45], Loss 0.30243928031583805\n",
            "0.18260744214057922\n",
            "Epoch 46 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 46], Loss 0.30025557553873655\n",
            "0.18004445731639862\n",
            "Epoch 47 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 47], Loss 0.29823611584384885\n",
            "0.180968776345253\n",
            "Epoch 48 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 48], Loss 0.29498279868325655\n",
            "0.1794617623090744\n",
            "Epoch 49 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 49], Loss 0.29290340423232336\n",
            "0.17870502173900604\n",
            "Epoch 50 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 50], Loss 0.2911553924414261\n",
            "0.17690028250217438\n",
            "Epoch 51 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 51], Loss 0.28962302963993897\n",
            "0.17664700746536255\n",
            "Epoch 52 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 52], Loss 0.2870800076684417\n",
            "0.17620207369327545\n",
            "Epoch 53 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 53], Loss 0.2853076090942794\n",
            "0.17598754167556763\n",
            "Epoch 54 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 54], Loss 0.28265418674917697\n",
            "0.17474105954170227\n",
            "Epoch 55 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 55], Loss 0.28104364239009083\n",
            "0.1742904931306839\n",
            "Epoch 56 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 56], Loss 0.2797645468451632\n",
            "0.17366299033164978\n",
            "Epoch 57 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 57], Loss 0.27858609167699616\n",
            "0.17104221880435944\n",
            "Epoch 58 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 58], Loss 0.2763123468311839\n",
            "0.17133161425590515\n",
            "Epoch 59 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 59], Loss 0.2743901750594817\n",
            "0.17034664750099182\n",
            "Epoch 60 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 60], Loss 0.2731054552273061\n",
            "0.1692189872264862\n",
            "Epoch 61 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 61], Loss 0.2712465550168074\n",
            "0.1694105714559555\n",
            "Epoch 62 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 62], Loss 0.26971305058417416\n",
            "0.16798976063728333\n",
            "Epoch 63 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 63], Loss 0.26803129665267855\n",
            "0.16913197934627533\n",
            "Epoch 64 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 64], Loss 0.26732388323387213\n",
            "0.16769954562187195\n",
            "Epoch 65 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 65], Loss 0.26541511028741316\n",
            "0.1674889773130417\n",
            "Epoch 66 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 66], Loss 0.2630536530309722\n",
            "0.16653263568878174\n",
            "Epoch 67 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 67], Loss 0.2632352827133331\n",
            "0.16532054543495178\n",
            "Epoch 68 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 68], Loss 0.26155707178565957\n",
            "0.16463114321231842\n",
            "Epoch 69 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 69], Loss 0.2597944750986268\n",
            "0.16443267464637756\n",
            "Epoch 70 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 70], Loss 0.25810321492958915\n",
            "0.16601943969726562\n",
            "Epoch 71 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 71], Loss 0.25716136123402633\n",
            "0.1648969203233719\n",
            "Epoch 72 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 72], Loss 0.2554091793597624\n",
            "0.16569380462169647\n",
            "Epoch 73 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 73], Loss 0.2544969728650597\n",
            "0.16402491927146912\n",
            "Epoch 74 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 74], Loss 0.253860686073261\n",
            "0.16295422613620758\n",
            "Epoch 75 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 75], Loss 0.25196396038595553\n",
            "0.16159865260124207\n",
            "Epoch 76 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 76], Loss 0.25114750950034037\n",
            "0.16117359697818756\n",
            "Epoch 77 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 77], Loss 0.24974789762743102\n",
            "0.16202087700366974\n",
            "Epoch 78 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 78], Loss 0.2495461693115994\n",
            "0.15911801159381866\n",
            "Epoch 79 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 79], Loss 0.2475790775718942\n",
            "0.1584789752960205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j1w3sUD_9cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_df, X, top_N):  \n",
        "  predicted_df = pd.DataFrame()\n",
        "  # prediction can be made only for users who are presented into train and test both\n",
        "  predicted_df['userId'] = list(set(test_df['userId'].unique()) & set(X['userId'].unique()))\n",
        "  # initialize column with nulls\n",
        "  predicted_df['rec_items'] = 0\n",
        "  # some tricks to assign lists to dataframe cells\n",
        "  predicted_df.set_index('userId', inplace=True)\n",
        "  predicted_df = predicted_df.astype('object')\n",
        "\n",
        "  for user in tqdm(test_df['userId'].unique()):\n",
        "    if user in train_df['userId'].unique():\n",
        "      users_id = torch.tensor([user_to_idx[user]]*train_df['itemId'].nunique())\n",
        "      items_id = torch.tensor(np.arange(0,train_df['itemId'].nunique()))\n",
        "\n",
        "      engine.model.eval()\n",
        "      if config['use_cuda'] is True:\n",
        "        result = engine.model(users_id.cuda(),items_id.cuda()).cpu()\n",
        "      else:\n",
        "        result = engine.model(users_id,items_id)\n",
        "\n",
        "      recommendations = np.argsort(result.detach().numpy(),axis=0)[::-1].flatten()\n",
        "      \n",
        "      best_items = [idx_to_item[item] for item in recommendations[~np.in1d(recommendations,X.loc[X['userId'] == user, 'itemIdx'].values)][:top_N]]\n",
        "    \n",
        "      # use at to assign list\n",
        "      predicted_df.at[user, 'rec_items'] = best_items\n",
        "  predicted_df.reset_index(inplace=True)\n",
        "  return predicted_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTB6FWGpdO_G",
        "colab_type": "code",
        "outputId": "464550b6-c03e-4944-9497-0b0db4916140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "predicted_df = predict(test_df, X, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 656/656 [00:03<00:00, 206.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx_sz8vgEdTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ground_truth_df = collect_ground_truth(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxN_OhKEk7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx7DO3I2QwvG",
        "colab_type": "code",
        "outputId": "5eaed20c-87ec-42ab-8bcf-eb577ee3c363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "for metric_func in [precision_at_k, recall_at_k, hit_rate_at_k, ndcg_at_k, average_precision_at_k]:\n",
        "  print('{:s}: {:f}'.format(metric_func.__name__, mean_metric(predicted_df, ground_truth_df,metric_func,k=k)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision_at_k: 0.076220\n",
            "recall_at_k: 0.037781\n",
            "hit_rate_at_k: 0.278963\n",
            "ndcg_at_k: 0.044083\n",
            "average_precision_at_k: 0.147497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1ZgQAPH3q17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = torch.nn.ReLU()(vector)\n",
        "            # vector = torch.nn.BatchNorm1d()(vector)\n",
        "            # vector = torch.nn.Dropout(p=0.5)(vector)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self):\n",
        "        \"\"\"Loading weights from trained GMF model\"\"\"\n",
        "        config = self.config\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
        "        self.embedding_user.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "\n",
        "class MLPEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(MLPEngine, self).__init__(config)\n",
        "        print(self.model)\n",
        "\n",
        "        if config['pretrain']:\n",
        "            self.model.load_pretrain_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McyxN5eD4GkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_config = {'alias': 'mlp_factor8neg4_bz256_166432168_pretrain_reg_0.0000001',\n",
        "              'num_epoch': 100,\n",
        "              'batch_size': 1024,  # 1024,\n",
        "              'optimizer': 'adam',\n",
        "              'adam_lr': 1e-3,\n",
        "              'num_users': X['userId'].nunique(),\n",
        "              'num_items': X['itemId'].nunique(),\n",
        "              'latent_dim': 8,\n",
        "              'num_negative': 4,\n",
        "              'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
        "              'l2_regularization': 0.0000001,  # MLP model is sensitive to hyper params\n",
        "              'use_cuda': False,\n",
        "              'device_id': 0,\n",
        "              'pretrain': False,\n",
        "              'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
        "              'model_dir':'checkpoints/{}_Epoch{}.model'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRs3MCqW4hQs",
        "colab_type": "code",
        "outputId": "980c41db-4e42-4ea3-fa35-161e5074a442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "config = mlp_config\n",
        "engine = MLPEngine(config)\n",
        "\n",
        "for epoch in range(config['num_epoch']):\n",
        "    print('Epoch {} starts !'.format(epoch))\n",
        "    print('-' * 80)\n",
        "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
        "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
        "    print(engine.evaluate(validation_data))\n",
        "    engine.save(config['alias'], epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (embedding_user): Embedding(671, 8)\n",
            "  (embedding_item): Embedding(8019, 8)\n",
            "  (fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
            "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
            "  )\n",
            "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (logistic): Sigmoid()\n",
            ")\n",
            "Epoch 0 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 0], Loss 0.5208151134769473\n",
            "0.22560973465442657\n",
            "Epoch 1 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 1], Loss 0.48198141399386357\n",
            "0.20245379209518433\n",
            "Epoch 2 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 2], Loss 0.4354194313903122\n",
            "0.19670188426971436\n",
            "Epoch 3 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 3], Loss 0.39572531266198396\n",
            "0.18076419830322266\n",
            "Epoch 4 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 4], Loss 0.3768550234909958\n",
            "0.18402588367462158\n",
            "Epoch 5 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 5], Loss 0.36769320092721675\n",
            "0.18164195120334625\n",
            "Epoch 6 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 6], Loss 0.360438997590788\n",
            "0.1834273487329483\n",
            "Epoch 7 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 7], Loss 0.35548914322810893\n",
            "0.17435374855995178\n",
            "Epoch 8 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 8], Loss 0.35087448226667084\n",
            "0.18225322663784027\n",
            "Epoch 9 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 9], Loss 0.34839182072332825\n",
            "0.18370899558067322\n",
            "Epoch 10 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 10], Loss 0.3452267456898647\n",
            "0.1756133884191513\n",
            "Epoch 11 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 11], Loss 0.3439459791049845\n",
            "0.17484287917613983\n",
            "Epoch 12 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 12], Loss 0.3416981145871424\n",
            "0.17155176401138306\n",
            "Epoch 13 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 13], Loss 0.3399002995111246\n",
            "0.1835952252149582\n",
            "Epoch 14 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 14], Loss 0.338226137875104\n",
            "0.1699533611536026\n",
            "Epoch 15 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 15], Loss 0.3370863751324229\n",
            "0.16632746160030365\n",
            "Epoch 16 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 16], Loss 0.3358722902328919\n",
            "0.1684342473745346\n",
            "Epoch 17 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 17], Loss 0.3345422574200813\n",
            "0.17546333372592926\n",
            "Epoch 18 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 18], Loss 0.3350224310317926\n",
            "0.15818214416503906\n",
            "Epoch 19 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 19], Loss 0.33248569787779386\n",
            "0.1669231355190277\n",
            "Epoch 20 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 20], Loss 0.33295329605231944\n",
            "0.1516352742910385\n",
            "Epoch 21 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 21], Loss 0.3313162321537997\n",
            "0.1731736809015274\n",
            "Epoch 22 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 22], Loss 0.3305467645854725\n",
            "0.1603834182024002\n",
            "Epoch 23 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 23], Loss 0.33051572999419715\n",
            "0.1692461371421814\n",
            "Epoch 24 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 24], Loss 0.3299367113978462\n",
            "0.1613442301750183\n",
            "Epoch 25 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 25], Loss 0.3284688835474594\n",
            "0.16824942827224731\n",
            "Epoch 26 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 26], Loss 0.328083528617842\n",
            "0.16764825582504272\n",
            "Epoch 27 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 27], Loss 0.3274662123668862\n",
            "0.16711492836475372\n",
            "Epoch 28 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 28], Loss 0.32683139160319413\n",
            "0.16640566289424896\n",
            "Epoch 29 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 29], Loss 0.32660331952888355\n",
            "0.18879325687885284\n",
            "Epoch 30 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 30], Loss 0.3263063189906005\n",
            "0.17261838912963867\n",
            "Epoch 31 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 31], Loss 0.32451073697886285\n",
            "0.17483143508434296\n",
            "Epoch 32 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 32], Loss 0.3240929565950129\n",
            "0.15471112728118896\n",
            "Epoch 33 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 33], Loss 0.3230122941433504\n",
            "0.16693846881389618\n",
            "Epoch 34 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 34], Loss 0.3221344825616628\n",
            "0.1621248722076416\n",
            "Epoch 35 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 35], Loss 0.3213869604916699\n",
            "0.1781993955373764\n",
            "Epoch 36 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 36], Loss 0.32041435088731546\n",
            "0.18152078986167908\n",
            "Epoch 37 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 37], Loss 0.31995520030854374\n",
            "0.16213712096214294\n",
            "Epoch 38 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 38], Loss 0.3188824060216414\n",
            "0.17151737213134766\n",
            "Epoch 39 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 39], Loss 0.3172824817948637\n",
            "0.1572885662317276\n",
            "Epoch 40 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 40], Loss 0.3164825428024506\n",
            "0.16054658591747284\n",
            "Epoch 41 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 41], Loss 0.31612386235796947\n",
            "0.162678062915802\n",
            "Epoch 42 starts !\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-af2b7a70d9e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_a_train_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_an_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-11fde4e5d172>\u001b[0m in \u001b[0;36mtrain_an_epoch\u001b[0;34m(self, train_loader, epoch_id)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mrating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrating\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m#print('[Training Epoch {}] Batch {}, Loss {}'.format(epoch_id, batch_id, loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-11fde4e5d172>\u001b[0m in \u001b[0;36mtrain_single_batch\u001b[0;34m(self, users, items, ratings)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mratings_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_TBx5KUdx9P",
        "colab_type": "code",
        "outputId": "7fbfa213-8e4e-44b6-9d85-51656275f839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "predicted_df = predict(test_df, X, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 656/656 [00:05<00:00, 117.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJuSXLWz5XOH",
        "colab_type": "code",
        "outputId": "0ce3b763-7046-4cc6-d989-aec025fe0969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "for metric_func in [precision_at_k, recall_at_k, hit_rate_at_k, ndcg_at_k, average_precision_at_k]:\n",
        "  print('{:s}: {:f}'.format(metric_func.__name__, mean_metric(predicted_df, ground_truth_df,metric_func,k=k)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision_at_k: 0.066768\n",
            "recall_at_k: 0.032217\n",
            "hit_rate_at_k: 0.269817\n",
            "ndcg_at_k: 0.038889\n",
            "average_precision_at_k: 0.144178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvrbbe_AfmRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(NeuMF, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim_mf = config['latent_dim_mf']\n",
        "        self.latent_dim_mlp = config['latent_dim_mlp']\n",
        "\n",
        "        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n",
        "        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1] + config['latent_dim_mf'], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "\n",
        "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
        "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
        "\n",
        "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self):\n",
        "        \"\"\"Loading weights from trained MLP model & GMF model\"\"\"\n",
        "        config = self.config\n",
        "        config['latent_dim'] = config['latent_dim_mlp']\n",
        "        mlp_model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            mlp_model.cuda()\n",
        "        resume_checkpoint(mlp_model, model_dir=config['pretrain_mlp'], device_id=config['device_id'])\n",
        "\n",
        "        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n",
        "        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data\n",
        "        for idx in range(len(self.fc_layers)):\n",
        "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
        "\n",
        "        config['latent_dim'] = config['latent_dim_mf']\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
        "        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
        "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)\n",
        "\n",
        "\n",
        "class NeuMFEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = NeuMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(NeuMFEngine, self).__init__(config)\n",
        "        print(self.model)\n",
        "\n",
        "        if config['pretrain']:\n",
        "            self.model.load_pretrain_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdJqIhX_gDIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neumf_config = {'alias': 'pretrain_neumf_factor8neg4',\n",
        "                'num_epoch': 200,\n",
        "                'batch_size': 1024,\n",
        "                'optimizer': 'sgd',\n",
        "                'sgd_lr': 1e-4,\n",
        "                'sgd_momentum': 0.9,\n",
        "                'num_users': X['userId'].nunique(),\n",
        "                'num_items': X['itemId'].nunique(),\n",
        "                'latent_dim_mf': 8,\n",
        "                'latent_dim_mlp': 8,\n",
        "                'num_negative': 4,\n",
        "                'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
        "                'l2_regularization': 0.01,\n",
        "                'use_cuda': True,\n",
        "                'device_id': 0,\n",
        "                'pretrain': True,\n",
        "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4-implict_Epoch79.model'),\n",
        "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_bz256_166432168_pretrain_reg_0.0000001_Epoch41.model'),\n",
        "                'model_dir':'checkpoints/{}_Epoch{}.model'\n",
        "                }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTdivi2hOFd",
        "colab_type": "code",
        "outputId": "e153aa3f-85ac-4c2d-afde-16c0de44c3bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "config = neumf_config\n",
        "engine = NeuMFEngine(config)\n",
        "\n",
        "for epoch in range(config['num_epoch']):\n",
        "    print('Epoch {} starts !'.format(epoch))\n",
        "    print('-' * 80)\n",
        "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
        "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
        "    print(engine.evaluate(validation_data))\n",
        "    engine.save(config['alias'], epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuMF(\n",
            "  (embedding_user_mlp): Embedding(671, 8)\n",
            "  (embedding_item_mlp): Embedding(8019, 8)\n",
            "  (embedding_user_mf): Embedding(671, 8)\n",
            "  (embedding_item_mf): Embedding(8019, 8)\n",
            "  (fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
            "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
            "  )\n",
            "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
            "  (logistic): Sigmoid()\n",
            ")\n",
            "Epoch 0 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 0], Loss 0.26835452231158197\n",
            "0.15162697434425354\n",
            "Epoch 1 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 1], Loss 0.26660744107402534\n",
            "0.15327616035938263\n",
            "Epoch 2 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 2], Loss 0.2640133325153396\n",
            "0.15349726378917694\n",
            "Epoch 3 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 3], Loss 0.26172297907262426\n",
            "0.1536284238100052\n",
            "Epoch 4 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 4], Loss 0.2608285615753635\n",
            "0.15325510501861572\n",
            "Epoch 5 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 5], Loss 0.259545444914725\n",
            "0.15291355550289154\n",
            "Epoch 6 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 6], Loss 0.25866827988519075\n",
            "0.1526779681444168\n",
            "Epoch 7 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 7], Loss 0.258137510193836\n",
            "0.15245166420936584\n",
            "Epoch 8 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 8], Loss 0.25710980791961197\n",
            "0.15234431624412537\n",
            "Epoch 9 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 9], Loss 0.2564445167867835\n",
            "0.152394637465477\n",
            "Epoch 10 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 10], Loss 0.25666067448161695\n",
            "0.152101069688797\n",
            "Epoch 11 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 11], Loss 0.25582659389592904\n",
            "0.15191908180713654\n",
            "Epoch 12 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 12], Loss 0.25577553886931204\n",
            "0.1517714112997055\n",
            "Epoch 13 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 13], Loss 0.2557277441640167\n",
            "0.15149129927158356\n",
            "Epoch 14 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 14], Loss 0.25516003304350693\n",
            "0.15184247493743896\n",
            "Epoch 15 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 15], Loss 0.25463771367319216\n",
            "0.15187032520771027\n",
            "Epoch 16 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 16], Loss 0.25572140734631754\n",
            "0.15138918161392212\n",
            "Epoch 17 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 17], Loss 0.2549780104803828\n",
            "0.15123876929283142\n",
            "Epoch 18 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 18], Loss 0.2549225953036705\n",
            "0.1511029452085495\n",
            "Epoch 19 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 19], Loss 0.25385434547532626\n",
            "0.15129363536834717\n",
            "Epoch 20 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 20], Loss 0.25426450573940895\n",
            "0.15115568041801453\n",
            "Epoch 21 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 21], Loss 0.25479887487438224\n",
            "0.15089522302150726\n",
            "Epoch 22 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 22], Loss 0.25380239110429026\n",
            "0.15112000703811646\n",
            "Epoch 23 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 23], Loss 0.2550372054531159\n",
            "0.15067309141159058\n",
            "Epoch 24 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 24], Loss 0.25443990300011143\n",
            "0.1506425142288208\n",
            "Epoch 25 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 25], Loss 0.25451596063674375\n",
            "0.15049514174461365\n",
            "Epoch 26 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 26], Loss 0.2542609691971523\n",
            "0.15032105147838593\n",
            "Epoch 27 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 27], Loss 0.253936024586008\n",
            "0.15031610429286957\n",
            "Epoch 28 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 28], Loss 0.25472236976335183\n",
            "0.15028764307498932\n",
            "Epoch 29 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 29], Loss 0.25431685655166264\n",
            "0.15019270777702332\n",
            "Epoch 30 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 30], Loss 0.2544043075546051\n",
            "0.15018431842327118\n",
            "Epoch 31 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 31], Loss 0.2550617348303837\n",
            "0.1498592346906662\n",
            "Epoch 32 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 32], Loss 0.25451822055827905\n",
            "0.15000541508197784\n",
            "Epoch 33 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 33], Loss 0.2544111427919703\n",
            "0.14980806410312653\n",
            "Epoch 34 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 34], Loss 0.2545230067321333\n",
            "0.14981508255004883\n",
            "Epoch 35 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 35], Loss 0.25521199545853257\n",
            "0.14965075254440308\n",
            "Epoch 36 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 36], Loss 0.25449103833475645\n",
            "0.1496172994375229\n",
            "Epoch 37 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 37], Loss 0.25470888016849846\n",
            "0.14960739016532898\n",
            "Epoch 38 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 38], Loss 0.25452602173374816\n",
            "0.1496412605047226\n",
            "Epoch 39 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 39], Loss 0.2548181154822881\n",
            "0.1496042013168335\n",
            "Epoch 40 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 40], Loss 0.25462870062452503\n",
            "0.149765744805336\n",
            "Epoch 41 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 41], Loss 0.25540607750064154\n",
            "0.14956867694854736\n",
            "Epoch 42 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 42], Loss 0.25552395060878236\n",
            "0.14945796132087708\n",
            "Epoch 43 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 43], Loss 0.25503829860054283\n",
            "0.1495484560728073\n",
            "Epoch 44 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 44], Loss 0.2559322624255774\n",
            "0.14927974343299866\n",
            "Epoch 45 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 45], Loss 0.2552150235151471\n",
            "0.1492607444524765\n",
            "Epoch 46 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 46], Loss 0.2553294600374931\n",
            "0.1493619680404663\n",
            "Epoch 47 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 47], Loss 0.25449666574289664\n",
            "0.14967390894889832\n",
            "Epoch 48 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 48], Loss 0.2560985481756627\n",
            "0.14954158663749695\n",
            "Epoch 49 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 49], Loss 0.2561807949370691\n",
            "0.14936944842338562\n",
            "Epoch 50 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 50], Loss 0.2562537238133692\n",
            "0.14936575293540955\n",
            "Epoch 51 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 51], Loss 0.25590812918183375\n",
            "0.14933998882770538\n",
            "Epoch 52 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 52], Loss 0.2563448072710572\n",
            "0.149413600564003\n",
            "Epoch 53 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 53], Loss 0.25594466555435047\n",
            "0.14938852190971375\n",
            "Epoch 54 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 54], Loss 0.2569786818329915\n",
            "0.14923860132694244\n",
            "Epoch 55 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 55], Loss 0.25654138677415594\n",
            "0.14926102757453918\n",
            "Epoch 56 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 56], Loss 0.25661729891567453\n",
            "0.1491575837135315\n",
            "Epoch 57 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 57], Loss 0.2573783278113621\n",
            "0.14896802604198456\n",
            "Epoch 58 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 58], Loss 0.25737334616416324\n",
            "0.14905676245689392\n",
            "Epoch 59 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 59], Loss 0.2569239435382297\n",
            "0.14910653233528137\n",
            "Epoch 60 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 60], Loss 0.2570979596942575\n",
            "0.14920251071453094\n",
            "Epoch 61 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 61], Loss 0.2579987889545857\n",
            "0.1490994244813919\n",
            "Epoch 62 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 62], Loss 0.25746278552706614\n",
            "0.1491866409778595\n",
            "Epoch 63 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 63], Loss 0.2574666651916363\n",
            "0.1493469625711441\n",
            "Epoch 64 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 64], Loss 0.25824065655909456\n",
            "0.14910314977169037\n",
            "Epoch 65 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 65], Loss 0.25835288959970165\n",
            "0.14918659627437592\n",
            "Epoch 66 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 66], Loss 0.25900172004833333\n",
            "0.14895910024642944\n",
            "Epoch 67 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 67], Loss 0.25925551869172964\n",
            "0.14877454936504364\n",
            "Epoch 68 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 68], Loss 0.25944709131675486\n",
            "0.14881090819835663\n",
            "Epoch 69 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 69], Loss 0.2593173763569126\n",
            "0.148916557431221\n",
            "Epoch 70 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 70], Loss 0.2598220632579123\n",
            "0.14887739717960358\n",
            "Epoch 71 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 71], Loss 0.25869828937327966\n",
            "0.14922764897346497\n",
            "Epoch 72 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 72], Loss 0.25948943602124497\n",
            "0.14916710555553436\n",
            "Epoch 73 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 73], Loss 0.25917737647495437\n",
            "0.14916421473026276\n",
            "Epoch 74 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 74], Loss 0.26071455305480673\n",
            "0.14905880391597748\n",
            "Epoch 75 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 75], Loss 0.2602214268175198\n",
            "0.14922896027565002\n",
            "Epoch 76 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 76], Loss 0.2601696270405367\n",
            "0.14930540323257446\n",
            "Epoch 77 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 77], Loss 0.261735448926951\n",
            "0.14912469685077667\n",
            "Epoch 78 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 78], Loss 0.26020205908820343\n",
            "0.14949671924114227\n",
            "Epoch 79 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 79], Loss 0.2604225510868703\n",
            "0.14972734451293945\n",
            "Epoch 80 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 80], Loss 0.2610389699714374\n",
            "0.14956559240818024\n",
            "Epoch 81 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 81], Loss 0.2607668454052776\n",
            "0.1496841162443161\n",
            "Epoch 82 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 82], Loss 0.26190808081345573\n",
            "0.14965181052684784\n",
            "Epoch 83 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 83], Loss 0.2626469890452416\n",
            "0.14945976436138153\n",
            "Epoch 84 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 84], Loss 0.2615818284605808\n",
            "0.14956587553024292\n",
            "Epoch 85 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 85], Loss 0.2621419622746892\n",
            "0.1496548056602478\n",
            "Epoch 86 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 86], Loss 0.2628294159177482\n",
            "0.14944860339164734\n",
            "Epoch 87 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 87], Loss 0.2629961796478536\n",
            "0.14964880049228668\n",
            "Epoch 88 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 88], Loss 0.26270213402302217\n",
            "0.14982867240905762\n",
            "Epoch 89 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 89], Loss 0.2631806177815153\n",
            "0.14977949857711792\n",
            "Epoch 90 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 90], Loss 0.26347352955545295\n",
            "0.14969316124916077\n",
            "Epoch 91 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 91], Loss 0.26428097811772056\n",
            "0.14969117939472198\n",
            "Epoch 92 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 92], Loss 0.26402384811973856\n",
            "0.14966294169425964\n",
            "Epoch 93 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 93], Loss 0.26396626168120224\n",
            "0.14982205629348755\n",
            "Epoch 94 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 94], Loss 0.26374664507081025\n",
            "0.14995872974395752\n",
            "Epoch 95 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 95], Loss 0.26454233793558274\n",
            "0.1499578058719635\n",
            "Epoch 96 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 96], Loss 0.263888932918377\n",
            "0.15016329288482666\n",
            "Epoch 97 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 97], Loss 0.26519291700690895\n",
            "0.15017002820968628\n",
            "Epoch 98 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 98], Loss 0.2650869872893556\n",
            "0.15013396739959717\n",
            "Epoch 99 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 99], Loss 0.26488831960170317\n",
            "0.15009468793869019\n",
            "Epoch 100 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 100], Loss 0.26611420947366055\n",
            "0.1500905603170395\n",
            "Epoch 101 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 101], Loss 0.2661422083159815\n",
            "0.15024042129516602\n",
            "Epoch 102 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 102], Loss 0.2664798608044256\n",
            "0.1502150446176529\n",
            "Epoch 103 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 103], Loss 0.26553991507288277\n",
            "0.15058499574661255\n",
            "Epoch 104 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 104], Loss 0.26619268509842303\n",
            "0.1506911963224411\n",
            "Epoch 105 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 105], Loss 0.26681989426985603\n",
            "0.15059733390808105\n",
            "Epoch 106 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 106], Loss 0.2666798297722431\n",
            "0.1506734937429428\n",
            "Epoch 107 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 107], Loss 0.26721336180481586\n",
            "0.15066438913345337\n",
            "Epoch 108 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 108], Loss 0.26761230980224665\n",
            "0.15075384080410004\n",
            "Epoch 109 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 109], Loss 0.26735195561496206\n",
            "0.15091168880462646\n",
            "Epoch 110 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 110], Loss 0.2683138870647875\n",
            "0.15086720883846283\n",
            "Epoch 111 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 111], Loss 0.2685022399840453\n",
            "0.15097428858280182\n",
            "Epoch 112 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 112], Loss 0.26847375067813556\n",
            "0.15117356181144714\n",
            "Epoch 113 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 113], Loss 0.2685587009233711\n",
            "0.15129172801971436\n",
            "Epoch 114 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 114], Loss 0.26923020082940746\n",
            "0.1513168215751648\n",
            "Epoch 115 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 115], Loss 0.2689811636564654\n",
            "0.15152548253536224\n",
            "Epoch 116 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 116], Loss 0.26975571036690454\n",
            "0.1514703333377838\n",
            "Epoch 117 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 117], Loss 0.26994159430880815\n",
            "0.15135550498962402\n",
            "Epoch 118 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 118], Loss 0.2698742208403472\n",
            "0.15152369439601898\n",
            "Epoch 119 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 119], Loss 0.271145743826146\n",
            "0.1513597071170807\n",
            "Epoch 120 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 120], Loss 0.27146530832688714\n",
            "0.15123040974140167\n",
            "Epoch 121 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 121], Loss 0.2708043515418483\n",
            "0.15162277221679688\n",
            "Epoch 122 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 122], Loss 0.2714044501384099\n",
            "0.1516440361738205\n",
            "Epoch 123 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 123], Loss 0.2711609734282733\n",
            "0.15193171799182892\n",
            "Epoch 124 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 124], Loss 0.27189104122749824\n",
            "0.15199989080429077\n",
            "Epoch 125 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 125], Loss 0.27267310401331357\n",
            "0.15205931663513184\n",
            "Epoch 126 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 126], Loss 0.2717367613737562\n",
            "0.15233924984931946\n",
            "Epoch 127 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 127], Loss 0.27292048109140365\n",
            "0.15234293043613434\n",
            "Epoch 128 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 128], Loss 0.27250630481053245\n",
            "0.15244805812835693\n",
            "Epoch 129 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 129], Loss 0.2740777786326619\n",
            "0.15215334296226501\n",
            "Epoch 130 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 130], Loss 0.2736324547490539\n",
            "0.15232351422309875\n",
            "Epoch 131 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 131], Loss 0.27385446890793014\n",
            "0.15250369906425476\n",
            "Epoch 132 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 132], Loss 0.2740707498120699\n",
            "0.15259799361228943\n",
            "Epoch 133 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 133], Loss 0.2741854428190046\n",
            "0.15270449221134186\n",
            "Epoch 134 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 134], Loss 0.27477980890808557\n",
            "0.15276412665843964\n",
            "Epoch 135 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 135], Loss 0.27453862561412967\n",
            "0.15297631919384003\n",
            "Epoch 136 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 136], Loss 0.27511547558603033\n",
            "0.1531895399093628\n",
            "Epoch 137 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 137], Loss 0.2756263747327799\n",
            "0.15316890180110931\n",
            "Epoch 138 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 138], Loss 0.2758063947820382\n",
            "0.15316523611545563\n",
            "Epoch 139 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 139], Loss 0.27651684170803137\n",
            "0.15318803489208221\n",
            "Epoch 140 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 140], Loss 0.27627436935374167\n",
            "0.15336106717586517\n",
            "Epoch 141 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 141], Loss 0.2772508308423304\n",
            "0.15343059599399567\n",
            "Epoch 142 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 142], Loss 0.2771420862558669\n",
            "0.15336564183235168\n",
            "Epoch 143 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 143], Loss 0.2777691883499292\n",
            "0.15345868468284607\n",
            "Epoch 144 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 144], Loss 0.2770464849331386\n",
            "0.15376707911491394\n",
            "Epoch 145 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 145], Loss 0.2775083486221533\n",
            "0.15395575761795044\n",
            "Epoch 146 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 146], Loss 0.27841422010136213\n",
            "0.15408067405223846\n",
            "Epoch 147 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 147], Loss 0.27909178253823674\n",
            "0.1541362702846527\n",
            "Epoch 148 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 148], Loss 0.2793515154481989\n",
            "0.15404073894023895\n",
            "Epoch 149 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 149], Loss 0.27888323665544346\n",
            "0.1542978584766388\n",
            "Epoch 150 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 150], Loss 0.27975285360946767\n",
            "0.15451978147029877\n",
            "Epoch 151 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 151], Loss 0.27975465379281733\n",
            "0.15457729995250702\n",
            "Epoch 152 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 152], Loss 0.28067805031232074\n",
            "0.1544315069913864\n",
            "Epoch 153 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 153], Loss 0.2807204134344703\n",
            "0.1545223742723465\n",
            "Epoch 154 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 154], Loss 0.28114188055900696\n",
            "0.15457004308700562\n",
            "Epoch 155 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 155], Loss 0.2812821156067834\n",
            "0.15481241047382355\n",
            "Epoch 156 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 156], Loss 0.2819253565375432\n",
            "0.15484677255153656\n",
            "Epoch 157 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 157], Loss 0.2814205896362091\n",
            "0.1551540642976761\n",
            "Epoch 158 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 158], Loss 0.28213889997968983\n",
            "0.1552606225013733\n",
            "Epoch 159 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 159], Loss 0.28301325018427015\n",
            "0.15514148771762848\n",
            "Epoch 160 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 160], Loss 0.2828899622403063\n",
            "0.15551228821277618\n",
            "Epoch 161 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 161], Loss 0.28309744200875275\n",
            "0.15560409426689148\n",
            "Epoch 162 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 162], Loss 0.28399296768647025\n",
            "0.15563464164733887\n",
            "Epoch 163 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 163], Loss 0.2835609055725874\n",
            "0.15589749813079834\n",
            "Epoch 164 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 164], Loss 0.28393255257676836\n",
            "0.15599867701530457\n",
            "Epoch 165 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 165], Loss 0.28478902121209115\n",
            "0.15595316886901855\n",
            "Epoch 166 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 166], Loss 0.2846304121094819\n",
            "0.1560603380203247\n",
            "Epoch 167 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 167], Loss 0.28507831349837043\n",
            "0.15623313188552856\n",
            "Epoch 168 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 168], Loss 0.2851698297930326\n",
            "0.15640421211719513\n",
            "Epoch 169 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 169], Loss 0.2860932792358342\n",
            "0.15635119378566742\n",
            "Epoch 170 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 170], Loss 0.28624680893259413\n",
            "0.15670128166675568\n",
            "Epoch 171 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 171], Loss 0.2867047051764519\n",
            "0.15672416985034943\n",
            "Epoch 172 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 172], Loss 0.2869992779243661\n",
            "0.15669022500514984\n",
            "Epoch 173 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 173], Loss 0.2867464557246121\n",
            "0.15700194239616394\n",
            "Epoch 174 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 174], Loss 0.2879407667832389\n",
            "0.15694141387939453\n",
            "Epoch 175 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 175], Loss 0.28801339258134895\n",
            "0.1570749580860138\n",
            "Epoch 176 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 176], Loss 0.28843876798596\n",
            "0.15715989470481873\n",
            "Epoch 177 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 177], Loss 0.28849764700132835\n",
            "0.15729108452796936\n",
            "Epoch 178 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 178], Loss 0.2889134799484658\n",
            "0.15749973058700562\n",
            "Epoch 179 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 179], Loss 0.28903019384296946\n",
            "0.15755118429660797\n",
            "Epoch 180 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 180], Loss 0.28936904097377025\n",
            "0.1577472984790802\n",
            "Epoch 181 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 181], Loss 0.2900564725947591\n",
            "0.15772894024848938\n",
            "Epoch 182 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 182], Loss 0.2903821252967756\n",
            "0.15791594982147217\n",
            "Epoch 183 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 183], Loss 0.29046231539024364\n",
            "0.15787814557552338\n",
            "Epoch 184 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 184], Loss 0.29086185002748943\n",
            "0.15816614031791687\n",
            "Epoch 185 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 185], Loss 0.2914325385670395\n",
            "0.15817636251449585\n",
            "Epoch 186 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 186], Loss 0.2922234645986979\n",
            "0.158152773976326\n",
            "Epoch 187 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 187], Loss 0.2922182939397199\n",
            "0.1582520604133606\n",
            "Epoch 188 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 188], Loss 0.2925688199764859\n",
            "0.15839387476444244\n",
            "Epoch 189 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 189], Loss 0.29303235123291127\n",
            "0.1585080474615097\n",
            "Epoch 190 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 190], Loss 0.2930887713369015\n",
            "0.15887576341629028\n",
            "Epoch 191 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 191], Loss 0.2936412731630612\n",
            "0.15888920426368713\n",
            "Epoch 192 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 192], Loss 0.2940750909765913\n",
            "0.15891879796981812\n",
            "Epoch 193 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 193], Loss 0.29397285134039797\n",
            "0.1592687964439392\n",
            "Epoch 194 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 194], Loss 0.2946143331429248\n",
            "0.15929001569747925\n",
            "Epoch 195 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 195], Loss 0.29472072215910156\n",
            "0.15947647392749786\n",
            "Epoch 196 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 196], Loss 0.29539430413977585\n",
            "0.15966695547103882\n",
            "Epoch 197 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 197], Loss 0.29612435545541543\n",
            "0.1593833863735199\n",
            "Epoch 198 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 198], Loss 0.2956850443441959\n",
            "0.15982697904109955\n",
            "Epoch 199 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 199], Loss 0.29612964233465955\n",
            "0.1600230485200882\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}